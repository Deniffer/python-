{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK包的安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***什么是NLTK***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个完整的自然语言处理框架\n",
    "* 自带语料库，词性分类库\n",
    "* 自带分类，分词，等等功能\n",
    "* 有强大的社区支持\n",
    "* 框架设计上没有考虑中文\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***NLTK的主要模块***\n",
    "\n",
    "![](./nltkm.png)\n",
    "\n",
    "\n",
    "### ***如何安装NLTK***\n",
    "Anaconda中已经默认安装\n",
    "* pip install nltk\n",
    "* nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***NLTK替代包***\n",
    "NLTK的使用稍显复杂，初学者也可以使用各种基于NLTK的简化包\n",
    "\n",
    "> TextBlob\n",
    "\n",
    "# 语料库的准备\n",
    "## 什么是语料库\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 布朗语料库示例\n",
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见的语料库格式\n",
    "外部文件\n",
    "\n",
    "除直接网络抓取并加工的情况外，原始文档由于内容较多，往往会首先以单个/多个文本文件的形式保存在外部，然后读入程序\n",
    "\n",
    "list\n",
    "\n",
    "结构灵活松散，有利于对原始语料进行处理，也可以随时增删成员\n",
    "> [\n",
    ">\n",
    ">  '大鱼吃小鱼也吃虾米，小鱼吃虾米。',\n",
    "> \n",
    "> '我帮你，你也帮我。'\n",
    ">\n",
    "> ]\n",
    "\n",
    "list of list\n",
    "\n",
    "语料完成分词后的常见形式，每个文档成为词条构成的list，而这些list又是原文档list的成员\n",
    "> [\n",
    "> \n",
    "> ['大鱼', '吃', '小鱼', '也', '吃', '虾米', '，', '小鱼', '吃', '虾米', '。'],\n",
    "> \n",
    "> ['我', '帮', '你', '，', '你', '也', '帮', '我', '。']\n",
    "> \n",
    "> ]\n",
    "\n",
    "DataFrame\n",
    "\n",
    "使用词袋模型进行后续数据分析时常见格式，行/列代表语料index，相应的列/行代表词条，或者需要加以记录的文档属性，如作者，原始超链接，发表日期等\n",
    "词条/文档对应时，单元格记录相应的词条出现频率，或者相应的概率/分值\n",
    "    Doc2Term矩阵\n",
    "    Term2Doc矩阵\n",
    "可以和原始语料的外部文件/list配合使用\n",
    "\n",
    "对于单个文档，也可以建立DataFrame，用行/列代表一个句子/段落/章节。\n",
    "\n",
    "## 准备《射雕》语料库\n",
    "为使用Python还不熟练的学员提供一个基于Pandas的通用操作框架。\n",
    "\n",
    "***读入为数据框***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 有的环境配置下read_table出错，因此改用read_csv\n",
    "raw = pd.read_csv(\"金庸-射雕英雄传txt精校版.txt\",\n",
    "                  names = ['txt'], sep ='aaa', encoding =\"utf-8\" ,engine='python')\n",
    "print(len(raw))\n",
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***加入章节标识***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 章节判断用变量预处理\n",
    "def m_head(tmpstr):\n",
    "    return tmpstr[:1]\n",
    "\n",
    "def m_mid(tmpstr):\n",
    "    return tmpstr.find(\"回 \")\n",
    "\n",
    "raw['head'] = raw.txt.apply(m_head)\n",
    "raw['mid'] = raw.txt.apply(m_mid)\n",
    "raw['len'] = raw.txt.apply(len)\n",
    "# raw['chap'] = 0\n",
    "raw.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 章节判断\n",
    "chapnum = 0\n",
    "for i in range(len(raw)):\n",
    "    if raw['head'][i] == \"第\" and raw['mid'][i] > 0 and raw['len'][i] < 30 :\n",
    "        chapnum += 1\n",
    "    if chapnum >= 40 and raw['txt'][i] == \"附录一：成吉思汗家族\" :\n",
    "        chapnum = 0\n",
    "    raw.loc[i, 'chap'] = chapnum\n",
    "    \n",
    "# 删除临时变量\n",
    "del raw['head']\n",
    "del raw['mid']\n",
    "del raw['len']\n",
    "raw.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***提取出所需章节***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw[raw.chap == 7]\n",
    "tmpchap = raw[raw.chap == 7].copy()\n",
    "\n",
    "##tmpchap = raw[raw.chap == 7].copy()\n",
    "#firstIdx = tmpchap.index[0]\n",
    "#firstIdx\n",
    "tmpchap.reset_index(drop=True, inplace=True)\n",
    "\n",
    "tmpchap['paraidx'] = tmpchap.index\n",
    "tmpchap['paralen'] = tmpchap.txt.apply(len)\n",
    "tmpchap[tmpchap.paralen == tmpchap.paralen.max()]\n",
    "#tmpchap.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmppara = tmpchap.txt[2]\n",
    "tmppara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pandas官网](https://pandas.pydata.org/)\n",
    "\n",
    "[pandas中文](https://www.pypandas.cn)\n",
    "\n",
    "[正则表达式](https://www.runoob.com/regexp/regexp-tutorial.html)\n",
    "\n",
    "[python正则表达](https://www.runoob.com/python/python-reg-expressions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tmppara = tmpchap[tmpchap['paraidx'] == 100].copy()\n",
    "tmppara\n",
    "#tmpstr = tmppara.txt[224]\n",
    "#tmpstr\n",
    "#sentences = re.findall('(.*?[？。！；：](’|”)?)',tmpstr)\n",
    "#sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "raw.txt.agg(len).plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawgrp = raw.groupby('chap')\n",
    "chapter = rawgrp.sum()##.agg(sum) # 只有字符串列的情况下，sum函数自动转为合并字符串\n",
    "chapter = chapter[chapter.index != 0]\n",
    "chapter.txt[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实战1：准备工具与素材\n",
    "请自行完成分析用Anaconda环境的安装和配置。\n",
    "\n",
    "请自行熟悉Jupyter notebook环境的操作。\n",
    "\n",
    "自行提取《射雕》任意一回的文字，并完成如下操作：\n",
    "\n",
    "* 将其读入为按整句分案例行的数据框格式，并用另一个变量标识其所在段落的流水号。\n",
    "* 将上述数据框转换为以整段为成员的list格式。\n",
    "\n",
    "说明：\n",
    "最后一题主要涉及到Pandas的操作，对该模块不熟悉的学员可直接继续后续课程的学习，这部分知识的欠缺并不会影响对文本挖掘课程本身的学习。当然，能懂得相应的知识是最好的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词\n",
    "## 分词原理简介\n",
    "\n",
    "### 分词的算法分类\n",
    "\n",
    "* 基于字符串的匹配\n",
    "  * 即扫描字符串，如果发现字符串的子串和词相同，就算匹配。\n",
    "  * 通常会加入一些启发式算法，比如“正向/反向最大匹配”，“长词优先”等\n",
    "  * 优点是速度快，但对歧义和未登录词处理不好\n",
    "  \n",
    "* 基于统计以及机器学习的分词方式\n",
    "  * 基于人工标注的词性和统计特征进行建模，并通过模型计算分词概率\n",
    "  * 常见的序列标注模型有HMM(隐马尔可夫)和CRF(条件随机场)\n",
    "  * 这类分词算法能很好的处理歧义和未登录词问题，效果比前一类要好，但是需要大量的人工标注数据，分词速度也较慢\n",
    "  \n",
    "**注意：分词算法本身在中文文本挖掘里就是一个“巨坑”**\n",
    "\n",
    "### 基于字符串匹配的分词算法原理\n",
    "\n",
    "* 以现有的词典为基础进行\n",
    "  \n",
    "* 最大匹配法：以设定的最大词长度为框架，取出其中最长的匹配词\n",
    "  * “中华人民共和国”会被完整取出，而不会进一步被分词\n",
    "  * 最佳匹配法：按照词典中的频率高低，优先取高频词\n",
    "* 最大概率法：对句子整体进行分词，找到最佳的词汇排列组合规律\n",
    "  * 例：早上好->早上/好\n",
    "* 最短路径分词：寻找单词数最少的分词方式\n",
    "\n",
    "### 分词的难点\n",
    "\n",
    "* 分词歧义\n",
    "  * 我个人没有意见\n",
    "  * 三个人没有意见\n",
    "* 未登录词识别：蔡国庆\n",
    "  * 数字\n",
    "  * 实体名称/专业术语\n",
    "  * 成语\n",
    "  * 虚词、语气词\n",
    "  \n",
    "### 常见的分词工具\n",
    "* 中科院计算所NLPIR [http://ictclas.nlpir.org/](http://ictclas.nlpir.org/)\n",
    "* 哈工大的LTP [https://www.ltp-cloud.com/](https://www.ltp-cloud.com/)\n",
    "* 斯坦福分词器 [http://nlp.stanford.edu/software/segmenter.shtml](http://nlp.stanford.edu/software/segmenter.shtml)\n",
    "* 结巴分词 [https://github.com/fxsjy/jieba](https://github.com/fxsjy/jieba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结巴分词的基本用法\n",
    "jieba是目前应用最广，评价也较高的分词工具包\n",
    "\n",
    "*安装*\n",
    "https://pypi.python.org/pypi/jieba/\n",
    "\n",
    "pip install jieba\n",
    "\n",
    "*基本特点*\n",
    "三种分词模式\n",
    "\n",
    "精确模式，试图将句子最精确地切开，适合做文本分析\n",
    "全模式，把句子中所有的可以成词的词语都扫描出来，速度非常快，但是不能解决歧义\n",
    "搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词\n",
    "\n",
    "支持繁体分词\n",
    "\n",
    "支持自定义词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "tmpstr = \"郭靖和哀牢山三十六剑。\"\n",
    "res = jieba.cut(tmpstr) # 精确模式\n",
    "\n",
    "print(res) # 是一个可迭代的 generator，可以使用 for 循环来遍历结果，本质上类似list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = jieba.cut(tmpstr)\n",
    "list(word for word in res) # 演示generator的用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jieba.lcut(tmpstr)) # 结果直接输出为list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('/'.join(jieba.cut(tmpstr, cut_all = True))) # 全模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搜索引擎模式，还有jieba.lcut_for_search可用\n",
    "print('/'.join(jieba.cut_for_search(tmpstr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw['new'] =jieba.lcut( raw.txt.)\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改词典\n",
    "### 动态增删新词\n",
    "在程序中可以动态根据分词的结果，对内存中的词库进行更新\n",
    "\n",
    "add_word(word)\n",
    "\n",
    "word：新词\n",
    "freq=None：词频\n",
    "tag=None：具体词性\n",
    "\n",
    "del_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 动态修改词典\n",
    "jieba.add_word(\"哀牢山三十六剑\")\n",
    "'/'.join(jieba.cut(tmpstr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.del_word(\"哀牢山三十六剑\")\n",
    "'/'.join(jieba.cut(tmpstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用自定义词典\n",
    "load_userdict(file_name)\n",
    "\n",
    "file_name：文件类对象或自定义词典的路径\n",
    "\n",
    "词典基本格式\n",
    "\n",
    "一个词占一行：词、词频（可省略）、词性（可省略），用空格隔开\n",
    "词典文件必须为 UTF-8 编码\n",
    "    必要时可以使用Uedit32进行文件编码转换\n",
    "\n",
    "> 云计算 5\n",
    ">\n",
    "> 李小福 2 nr\n",
    "> \n",
    ">  easy_install 3 eng\n",
    "> \n",
    "> 台中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = '金庸小说词库.txt'\n",
    "jieba.load_userdict(dict) # dict为自定义词典的路径\n",
    "'/'.join(jieba.cut(tmpstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用搜狗细胞词库\n",
    "[https://pinyin.sogou.com/dict/](https://pinyin.sogou.com/dict/)\n",
    "\n",
    "按照词库分类或者关键词搜索方式，查找并下载所需词库\n",
    "\n",
    "使用转换工具，将其转换为txt格式\n",
    "\n",
    "深蓝词库转换\n",
    "奥创词库转换\n",
    "\n",
    "在程序中导入相应词库\n",
    "\n",
    "## 去除停用词\n",
    "### 常见的停用词种类\n",
    "超高频的常用词：基本不携带有效信息/歧义太多无分析价值\n",
    "\n",
    "> 的、地、得\n",
    "\n",
    "虚词：如介词，连词等\n",
    "\n",
    "> 只、条、件\n",
    "> 当、从、同\n",
    "\n",
    "\n",
    "专业领域的高频词：基本不携带有效信息\n",
    "\n",
    "视情况而定的停用词\n",
    "\n",
    "> 呵呵\n",
    "> emoj\n",
    "\n",
    "### 分词后去除停用词\n",
    "基本步骤\n",
    "\n",
    "读入停用词表文件\n",
    "正常分词\n",
    "在分词结果中去除停用词\n",
    "\n",
    "> 新列表 = [ word for word in 源列表 if word not in 停用词列表 ]\n",
    "\n",
    "该方法存在的问题：停用词必须要被分词过程正确拆分出来才行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlist = [ w for w in jieba.cut(tmpstr) if w not in ['和', '。'] ] \n",
    "print(newlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tmpdf = pd.read_csv('停用词.txt',\n",
    "                    names = ['w'], sep = 'aaa', encoding = 'utf-8')\n",
    "tmpdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tmpchap = raw[raw.chap == 7].copy()\n",
    "\n",
    "##tmpchap = raw[raw.chap == 7].copy()\n",
    "#firstIdx = tmpchap.index[0]\n",
    "#firstIdx\n",
    "tmpchap.reset_index(drop=True, inplace=True)\n",
    "\n",
    "tmpchap['paraidx'] = tmpchap.index\n",
    "tmpchap['paralen'] = tmpchap.txt.apply(len)\n",
    "paratxt = tmpchap[tmpchap.paralen == tmpchap.paralen.max()].txt\n",
    "paratxt.reset_index(drop=True, inplace=True)\n",
    "# 熟悉Python的可以直接使用 open('stopWord.txt').readlines（） 获取停用词list，效率更高\n",
    "[ w for w in jieba.cut(paratxt[0]) if w not in list(tmpdf.w) ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用extract_tags函数去除停用词\n",
    "方法特点：\n",
    "\n",
    "根据TF-IDF算法将特征词提取出来，在提取之前去掉停用词\n",
    "可以人工指定停用词字典\n",
    "jieba.analyse.set_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用预先准备的停用词表\n",
    "import jieba.analyse as ana\n",
    "ana.set_stop_words('停用词.txt')\n",
    "jieba.lcut(tmpstr) # 读入的停用词列表对分词结果无效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana.extract_tags(tmpstr, topK = 20) # 使用TF-IDF算法提取关键词，并同时去掉停用词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词性标注\n",
    "import jieba.posseg\n",
    "\n",
    "posseg.cut()：给出附加词性的分词结果\n",
    "\n",
    "词性标注采用和 ICTCLAS 兼容的标记法\n",
    "\n",
    "后续可基于词性做进一步处理，如只提取出名词，动词等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as psg\n",
    "\n",
    "tmpres = psg.cut(tmpstr) # 附加词性的分词结果\n",
    "print(tmpres)\n",
    "\n",
    "for item in tmpres:\n",
    "    print(item.word, item.flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psg.lcut(tmpstr) # 直接输出为list，成员为pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词的NLTK实现\n",
    "NLTK只能识别用空格作为词条分割方式，因此不能直接用于中文文本的分词。\n",
    "\n",
    "一般的做法是先用jieba分词，然后转换为空格分隔的连续文本，再转入NLTK框架使用。\n",
    "\n",
    "rawtext = '周伯通笑道：“你懂了吗？...”\n",
    "txt = ' '.join(jieba.cut(rawtext)) # \"周伯通 笑 道 ：...\"\n",
    "toke = nltk.word_tokenize(txt) # ['周伯通', '笑', '道', '：'...]\n",
    "\n",
    "## 实战2：《射雕》一书分词\n",
    "选取第一回的文字，应用搜狗的细胞词库和停用词表，清理出干净的分词结果。\n",
    "\n",
    "选取第一回中最长的1个段落，比较不使用词库、不使用停用词表前后的分词结果。\n",
    "\n",
    "熟悉搜狗细胞词库网站中的资源，思考哪些词库可能是自己需要的，下载相应的资源并进行格式转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tmpchap = raw[raw.chap == 1].copy()\n",
    "\n",
    "##tmpchap = raw[raw.chap == 1].copy()\n",
    "#firstIdx = tmpchap.index[0]\n",
    "#firstIdx\n",
    "tmpchap.reset_index(drop=True, inplace=True)\n",
    "\n",
    "tmpchap['paraidx'] = tmpchap.index\n",
    "tmpchap['paralen'] = tmpchap.txt.apply(len)\n",
    "paratxt = tmpchap[tmpchap.paralen == tmpchap.paralen.max()].txt\n",
    "paratxt.reset_index(drop=True, inplace=True)\n",
    "# 熟悉Python的可以直接使用 open('stopWord.txt').readlines（） 获取停用词list，效率更高\n",
    "[ w for w in jieba.cut(paratxt[0]) if w not in list(tmpdf.w) ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词云展示\n",
    "## 词频统计\n",
    "\n",
    "绝大部分词频统计工具都是基于分词后构建词条的list进行，因此首先需要完成相应的分词工作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 载入语料\n",
    "raw = pd.read_csv(\"金庸-射雕英雄传txt精校版.txt\",\n",
    "                  names = ['txt'], sep ='aaa', encoding =\"utf-8\" ,engine='python')\n",
    "\n",
    "# 章节判断用变量预处理\n",
    "def m_head(tmpstr):\n",
    "    return tmpstr[:1]\n",
    "\n",
    "def m_mid(tmpstr):\n",
    "    return tmpstr.find(\"回 \")\n",
    "\n",
    "raw['head'] = raw.txt.apply(m_head)\n",
    "raw['mid'] = raw.txt.apply(m_mid)\n",
    "raw['len'] = raw.txt.apply(len)\n",
    "# 章节判断\n",
    "chapnum = 0\n",
    "for i in range(len(raw)):\n",
    "    if raw['head'][i] == \"第\" and raw['mid'][i] > 0 and raw['len'][i] < 30 :\n",
    "        chapnum += 1\n",
    "    if chapnum >= 40 and raw['txt'][i] == \"附录一：成吉思汗家族\" :\n",
    "        chapnum = 0\n",
    "    raw.loc[i, 'chap'] = chapnum\n",
    "    \n",
    "# 删除临时变量\n",
    "del raw['head']\n",
    "del raw['mid']\n",
    "del raw['len']\n",
    "\n",
    "#取出特定的某一回\n",
    "chapidx = 1\n",
    "raw[raw.chap == chapidx]\n",
    "tmpchap = raw[raw.chap == chapidx].copy()\n",
    "\n",
    "##tmpchap = raw[raw.chap == 7].copy()\n",
    "#firstIdx = tmpchap.index[0]\n",
    "#firstIdx\n",
    "tmpchap.reset_index(drop=True, inplace=True)\n",
    "\n",
    "tmpchap['paraidx'] = tmpchap.index\n",
    "tmpchap['paralen'] = tmpchap.txt.apply(len)\n",
    "#tmpchap[tmpchap.paralen == tmpchap.paralen.max()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltxt = \"\".join(tmpchap.txt[1:])\n",
    "alltxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "dict = '金庸小说词库.txt'\n",
    "jieba.load_userdict(dict) # dict为自定义词典的路径\n",
    "tmpdf = pd.read_csv('停用词.txt',\n",
    "                    names = ['w'], sep = 'aaa', encoding = 'utf-8',engine='python')\n",
    "#分词\n",
    "#word_list = jieba.lcut(chapter.txt[1])\n",
    "#word_list[:10]\n",
    "word_list = [ w for w in jieba.cut(alltxt) if w not in list(tmpdf.w) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建完list之后，也可以自行编写词频统计程序，框架如下：\n",
    "\n",
    "遍历整个list，对每个词条:\n",
    "\n",
    "> if word in d:\n",
    ">\n",
    ">    d[word] += 1\n",
    ">\n",
    "> else:\n",
    ">\n",
    ">    d[word] = 1\n",
    ">\n",
    "### 使用Pandas统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(word_list, columns = ['word'])\n",
    "result = df.groupby(['word']).size()\n",
    "print(type(result))\n",
    "freqlist = result.sort_values(ascending=False)\n",
    "freqlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqlist[freqlist.index == '道']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqlist[freqlist.index == '道'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqlist[freqlist.index == '黄蓉道']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用NLTK统计\n",
    "NLTK生成的结果为频数字典，在和某些程序包对接时比较有用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "fdist = nltk.FreqDist(word_list) # 生成完整的词条频数字典\n",
    "fdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 带上某个单词, 可以看到它在整个文章中出现的次数\n",
    "fdist['颜烈']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.keys() # 列出词条列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.tabulate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词云概述¶\n",
    "#### wordcloud包的安装\n",
    "*安装*\n",
    "\n",
    "docker的kaggle下推荐安装方法：\n",
    "\n",
    ">    conda install -c conda-forge wordcloud\n",
    "\n",
    "\n",
    "常规安装方法（docker或windows下）：\n",
    "\n",
    ">    pip install wordcloud\n",
    "\n",
    "    警告：常规方法安装wordcloud有可能非常顺利，也有可能会出各种问题\n",
    "\n",
    "***中文字体支持***\n",
    "\n",
    ".WordCloud(font_path='msyh.ttf')\n",
    "\n",
    "    需要带路径写完整字体文件名\n",
    "    注意Win10的字体文件后缀可能不一样\n",
    "\n",
    "\n",
    "### 绘制词云\n",
    "#### WordCloud的基本语法\n",
    "\n",
    "> class wordcloud.WordCloud(\n",
    ">\n",
    ">font_path, \n",
    ">\n",
    ">width, \n",
    ">\n",
    ">height, \n",
    ">\n",
    ">max_words, \n",
    ">\n",
    ">stopwords, \n",
    ">\n",
    ">min_font_size, \n",
    ">\n",
    ">font_step, \n",
    ">\n",
    ">relative_scaling, \n",
    ">\n",
    ">prefer_horizontal,\n",
    ">\n",
    ">background_color,\n",
    ">\n",
    ">mode,\n",
    ">\n",
    ">color_func,\n",
    ">\n",
    ">mask\n",
    ">\n",
    ">)\n",
    "\n",
    "\n",
    "常用功能：\n",
    "\n",
    "    font_path : 在图形中使用的字体，默认使用系统字体 \n",
    "    width / height = 200 : 图形的宽度/高度\n",
    "    max_words = 200 : 需要绘制的最多词条数\n",
    "    stopwords = None : 停用词列表，不指定时会使用系统默认停用词列表\n",
    "\n",
    "字体设定：\n",
    "\n",
    "    min_font_size = 4 /  max_font_size = None : 字符大小范围\n",
    "    font_step = 1 : 字号增加的步长\n",
    "    relative_scaling = .5: 词条频数比例和字号大小比例的换算关系，默认为50%\n",
    "    prefer_horizontal = 0.90 : 图中词条水平显示的比例\n",
    "\n",
    "颜色设定：\n",
    "\n",
    "    background_color = ”black” : 图形背景色\n",
    "    mode = ”RGB”: 图形颜色编码，如果指定为\"RGBA\"且背景色为None时，背景色为透明\n",
    "    color_func = None : 生成新颜色的函数，使用matplotlib的colormap\n",
    "\n",
    "背景掩模：\n",
    "\n",
    "    mask = None : 词云使用的背景图（遮罩）\n",
    "\n",
    "\n",
    "#### 用原始文本直接分词并绘制\n",
    "cloudobj = WordCloud().generate(text)\n",
    "\n",
    "    generate实际上是generate_from_text的别名\n",
    "文本需要用空格/标点符号分隔单词，否则不能正确分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "myfont = 'msyh.ttf'\n",
    "text = 'this is shanghai, 郭靖, 和, 哀牢山 三十六剑'\n",
    "cloudobj = wordcloud.WordCloud(font_path = myfont).generate(text)  \n",
    "print(cloudobj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*显示词云*\n",
    "> import matplotlib.pyplot as plt\n",
    ">\n",
    "> plt.imshow(cloudobj)\n",
    ">\n",
    "> plt.axis(\"off\")\n",
    ">\n",
    ">plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(cloudobj)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更改词云参数设定\n",
    "cloudobj = wordcloud.WordCloud(font_path = myfont, \n",
    "    width = 360, height = 180,\n",
    "    mode = \"RGBA\", background_color = None).generate(text)  \n",
    "\n",
    "plt.imshow(cloudobj)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*保存词云*\n",
    "\n",
    "wordcloud.to_file(保存文件的路径与名称) 该命令保存的是高精度图形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudobj.to_file(\"词云.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***生成射雕第一章的词云***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudobj = wordcloud.WordCloud(font_path = myfont, \n",
    "    width = 1200, height = 800,\n",
    "    mode = \"RGBA\", background_color = None).generate(' '.join(word_list)) \n",
    "\n",
    "plt.imshow(cloudobj)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudobj.to_file(\"词云2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于分词频数绘制\n",
    "generate()的实际操作\n",
    "\n",
    "    调用分词函数process_text()\n",
    "    调用基于频数的绘制函数fit_words()\n",
    "\n",
    "fit_words(dict)\n",
    "\n",
    "    实际上是generate_from_frequencies的别名\n",
    "    Dict: 由词条和频数构成的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基于分词频数绘制词云\n",
    "txt_freq = {'张三':100,'李四':90,'王二麻子':50}\n",
    "cloudobj = wordcloud.WordCloud(font_path = myfont).fit_words(txt_freq)\n",
    "\n",
    "plt.imshow(cloudobj)\n",
    "plt.axis(\"off\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***用频数生成射雕第一章的词云***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudobj = wordcloud.WordCloud(font_path = myfont).fit_words(fdist)\n",
    "\n",
    "plt.imshow(cloudobj)\n",
    "plt.axis(\"off\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词云的美化\n",
    "各种详细的操作和设定可以参考官网的案例：\n",
    "\n",
    "    https://amueller.github.io/word_cloud/\n",
    "\n",
    "#### 设置背景图片\n",
    "Mask / 掩模 / 遮罩\n",
    "\n",
    "    用于控制词云的整体形状\n",
    "    指定mask后，设置的宽高值将被忽略，遮罩形状被指定图形的形状取代。除全白的部分仍然保留外，其余部分会用于绘制词云。因此背景图片的画布一定要设置为白色（#FFFFFF）\n",
    "    字的大小，布局和颜色也会基于Mask生成\n",
    "    必要时需要调整颜色以增强可视效果\n",
    "\n",
    "基本调用方式\n",
    "\n",
    "    from scipy.misc import imread\n",
    "    mask = imread(背景图片名称)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imread\n",
    "\n",
    "\n",
    "cloudobj = wordcloud.WordCloud(font_path = myfont, \n",
    "    mask = imread(\"射雕背景1.png\"), \n",
    "    mode = \"RGBA\", background_color = None\n",
    "    ).generate(' '.join(word_list)) \n",
    "\n",
    "plt.imshow(cloudobj)\n",
    "plt.axis(\"off\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指定图片色系\n",
    "读取指定图片的色系设定\n",
    "\n",
    "    imgarray = np.array(imread(imgfilepath))\n",
    "\n",
    "获取图片颜色\n",
    "\n",
    "    bimgColors = wordcloud.ImageColorGenerator(imgarray)\n",
    "\n",
    "重置词云颜色\n",
    "\n",
    "    cloudobj.recolor(color_func=bimgColors)\n",
    "    # 利用已有词云对象直接重绘颜色，输出速度要比全部重绘快的多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "imgobj = imread(\"射雕背景2.png\")\n",
    "image_colors = wordcloud.ImageColorGenerator(np.array(imgobj))\n",
    "cloudobj.recolor(color_func=image_colors)\n",
    "\n",
    "plt.imshow(cloudobj)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 指定单词组颜色\n",
    "理想的状况应该是分组比较词频，在两组中都高频的词条在图形中相互抵消。\n",
    "\n",
    "Python目前只能实现词条分组上色。\n",
    "\n",
    "> color_to_words = {\n",
    ">\n",
    ">\n",
    ">'#00ff00': ['颜烈', '武官', '金兵', '小人'],\n",
    ">\n",
    ">'red': ['包惜弱', '郭啸天', '杨铁心', '丘处机']\n",
    ">\n",
    ">\n",
    ">} '#00ff00'为绿色的代码\n",
    ">\n",
    ">default_color = 'grey' # 其余单词的默认颜色\n",
    ">\n",
    ">cloudobj.recolor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 官网提供的颜色分组类代码，略有修改\n",
    "from wordcloud import get_single_color_func\n",
    "\n",
    "class GroupedColorFunc(object):\n",
    "\n",
    "    def __init__(self, color_to_words, default_color):\n",
    "        self.color_func_to_words = [\n",
    "            (get_single_color_func(color), set(words))\n",
    "            for (color, words) in color_to_words.items()]\n",
    "\n",
    "        self.default_color_func = get_single_color_func(default_color)\n",
    "\n",
    "    def get_color_func(self, word):\n",
    "        \"\"\"Returns a single_color_func associated with the word\"\"\"\n",
    "        try:\n",
    "            color_func = next(\n",
    "                color_func for (color_func, words) in self.color_func_to_words\n",
    "                if word in words)\n",
    "        except StopIteration:\n",
    "            color_func = self.default_color_func\n",
    "\n",
    "        return color_func\n",
    "\n",
    "    def __call__(self, word, **kwargs):\n",
    "        return self.get_color_func(word)(word, **kwargs)\n",
    "\n",
    "######\n",
    "\n",
    "# 指定分组色系\n",
    "color_to_words = {\n",
    "    '#00ff00': ['颜烈', '武官', '金兵', '官兵'],\n",
    "    'red': ['包惜弱', '郭啸天', '杨铁心', '丘处机']\n",
    "}\n",
    "\n",
    "default_color = 'grey' # 指定其他词条的颜色\n",
    "\n",
    "grouped_color_func = GroupedColorFunc(color_to_words, default_color)\n",
    "\n",
    "cloudobj.recolor(color_func=grouped_color_func)\n",
    "\n",
    "plt.imshow(cloudobj)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战3：优化射雕词云\n",
    "尝试进一步清理分词结果，并且只保留所有的名称（人名、地名）。\n",
    "\n",
    "    提示：可以使用词性标注功能，只保留名词和未知词性的词。\n",
    "         可以考虑对自定义词典做优化，通过强行调整权重等方法改善分词效果。\n",
    "\n",
    "将所有的人名按照蓝色系，地名按照红色系进行词云绘制。\n",
    "\n",
    "自行制作两个纯色图片，分别为绿色和蓝色，然后将其分别指定为绘图所用的色系，观察图形效果。\n",
    "\n",
    "尝试使用不同的背景图片作为掩模，思考怎样的图片才能使得绘图效果最佳。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文档信息的向量化\n",
    "\n",
    "所谓文档信息的向量化，就是将文档信息***数值化*** ，从而便于进行建模分析。\n",
    "\n",
    "## 词袋模型(One-hot表示方式)\n",
    "\n",
    "* 几乎是最早的用于提取文本特征的方法\n",
    "* 将文本直接简化为一系列词的集合\n",
    "  * 不考虑其语法和词序关系，每个词都是独立的 \n",
    "* 举例：\n",
    "  1. 对语料进行清理，并完成分词\n",
    "    * 大鱼/吃/小鱼/也/吃/虾米，小鱼吃虾米。\n",
    "  2. 对每个词进行编号，形成字典（顺序无关的流水号即可）\n",
    "    * {\"大鱼\":1,\"吃\":2,\"小鱼\":3,\"也\":4,\"虾米\"：5}\n",
    "  3. 用0/1代表该词是否在文本中出现，从而将文本记录为一个特征向量\n",
    "    * 大鱼吃小鱼也吃虾米 ->\\[大鱼,吃,小鱼,也,虾米\\]->\\[1,2,1,1,1\\]\n",
    "    * 小鱼吃虾米 ->\\[小鱼,吃,虾米\\]->\\[0,1,1,0,1\\]\n",
    "* 该方式也被称为词袋模型，Bag of Words，BOW\n",
    "  * 词和文本的关联就相当于文本是一个袋子，词只是直接装在袋子里\n",
    "* 显然，词袋模型是比较简单的模型，对文本中的信息有较多丢失，但已经可以解决很多实际问题\n",
    "  * 词袋模型的提出最初是为了解决文档分类问题，目前主要应用在NLP(Natural Language Process)，IR(Information Retrival)，CV(Computer Vision)等领域\n",
    "* 也可以不考虑词频，减少模型复杂度\n",
    "  * 词集模型：Set Of Words，单词构成的集合，常见于短文本分析\n",
    "  * 大鱼吃小鱼也吃虾米 ->\\[大鱼,吃,小鱼,也,虾米\\]->\\[1,**1**,1,1,1\\]\n",
    "* 优点：\n",
    "  * 解决了分类器不好处理离散数据的问题\n",
    "  * 在一定程度上也起到了扩充特征的作用\n",
    "* 缺点：\n",
    "  * 不考虑词与词之间的顺序\n",
    "  * 它假设词与词之间相互独立（在大多数情况下，词与词是相互有关联的）\n",
    "    * 老公 vs 老婆，老婆 vs 孩子他妈\n",
    "  * 它得到的特征是离散稀疏的（维度灾难）\n",
    "    * 每个词都是茫茫\"0\"海中的一个1：\\[0 0 0 0 0 **1** 0 0 0 0 0 0 ...\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词袋模型的gensim实现\n",
    "\n",
    "***gensim的安装***\n",
    "\n",
    "> pip install gensim\n",
    "> \n",
    "> 或者\n",
    "> \n",
    "> conda install gensim\n",
    "\n",
    "安装完成后如果使用word2vec时报错，建议去gensim官网下载MS windows install的exe程序进行安装：\n",
    "\n",
    "[https://pypi.python.org/pypi/gensim](https://pypi.python.org/pypi/gensim)\n",
    "\n",
    "***建立字典***\n",
    "\n",
    "Dictionary类用于建立word<->id映射关系，把所有单词取一个set()，并对set中每个单词分配一个Id号的map\n",
    "\n",
    "> class gensim.corpora.dictionary.Dictionary(\n",
    "> \n",
    "> documents=None : 若干个被拆成单词集合的文档的集合，一般以list in list形式出现\n",
    "> prune_at=2000000 : 字典中的最大词条容量\n",
    "> )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "texts = [['human', 'interface', 'computer']]\n",
    "dct = Dictionary(texts)  # fit dictionary\n",
    "dct.num_nnz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Dictionary类的属性***\n",
    "\n",
    "> token2id\n",
    "> \n",
    "> dict of (str, int) – token -> tokenId.\n",
    "> \n",
    "> id2token\n",
    "> \n",
    "> dict of (int, str) – Reverse mapping for token2id, initialized in lazy manner to > save memory.\n",
    "> \n",
    "> dfs\n",
    "> \n",
    "> dict of (int, int) – Document frequencies: token_id -> in how many documents > > > contain this token.\n",
    "> \n",
    "> num_docs\n",
    "> \n",
    "> int – Number of documents processed.\n",
    "> \n",
    "> num_pos\n",
    "> \n",
    "> int – Total number of corpus positions (number of processed words).\n",
    "> \n",
    "> num_nnz\n",
    "> \n",
    "> int – Total number of non-zeroes in the BOW matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向字典增加词条\n",
    "dct.add_documents([[\"cat\", \"say\", \"meow\"], [\"dog\"]])  \n",
    "dct.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换为BOW稀疏向量\n",
    "\n",
    "> dct.doc2bow( # 转换为BOW格式：list of (token_id, token_count)\n",
    "> \n",
    "> document : 用于转换的词条list\n",
    "> allow_update = False : 是否直接更新所用字典\n",
    "> return_missing = False : 是否返回新出现的（不在字典中的）词\n",
    "> )\n",
    "\n",
    "输出结果\n",
    "\n",
    "[(0, 2), (1, 2)]，表明在文档中id为0,1的词汇各出现了2次，至于其他词汇则没有出现\n",
    "return_missing = True时，输出list of (int, int), dict of (str, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct.doc2bow([\"this\", \"is\", \"cat\", \"not\", \"a\", \"dog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct.doc2bow([\"this\", \"is\", \"cat\", \"not\", \"a\", \"dog\"], return_missing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***转换为BOW长向量***\n",
    "可考虑的思路：\n",
    "\n",
    "    从稀疏格式自行转换。\n",
    "    直接生成文档-词条矩阵。\n",
    "\n",
    "doc2idx( # 转换为list of token_id\n",
    "\n",
    "    document : 用于转换的词条list\n",
    "    unknown_word_index = -1 : 为不在字典中的词条准备的代码\n",
    "\n",
    "输出结果\n",
    "\n",
    "    按照输入list的顺序列出所出现的各词条ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct.doc2idx([\"this\", \"is\", \"a\", \"dog\", \"not\", \"cat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成文档-词条矩阵\n",
    "#### 用Pandas库实现\n",
    "基本程序框架：\n",
    "\n",
    "    原始文档分词并清理\n",
    "    拼接为同一个dataframe\n",
    "    汇总并转换为文档-词条矩阵格式\n",
    "    去除低频词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 载入语料\n",
    "raw = pd.read_csv(\"金庸-射雕英雄传txt精校版.txt\",\n",
    "                  names = ['txt'], sep ='aaa', encoding =\"utf-8\" ,engine='python')\n",
    "\n",
    "# 章节判断用变量预处理\n",
    "def m_head(tmpstr):\n",
    "    return tmpstr[:1]\n",
    "\n",
    "def m_mid(tmpstr):\n",
    "    return tmpstr.find(\"回 \")\n",
    "\n",
    "raw['head'] = raw.txt.apply(m_head)\n",
    "raw['mid'] = raw.txt.apply(m_mid)\n",
    "raw['len'] = raw.txt.apply(len)\n",
    "# 章节判断\n",
    "chapnum = 0\n",
    "for i in range(len(raw)):\n",
    "    if raw['head'][i] == \"第\" and raw['mid'][i] > 0 and raw['len'][i] < 30 :\n",
    "        chapnum += 1\n",
    "    if chapnum >= 40 and raw['txt'][i] == \"附录一：成吉思汗家族\" :\n",
    "        chapnum = 0\n",
    "    raw.loc[i, 'chap'] = chapnum\n",
    "    \n",
    "# 删除临时变量\n",
    "del raw['head']\n",
    "del raw['mid']\n",
    "del raw['len']\n",
    "\n",
    "rawgrp = raw.groupby('chap')\n",
    "chapter = rawgrp.agg(sum) # 只有字符串列的情况下，sum函数自动转为合并字符串\n",
    "chapter = chapter[chapter.index != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定分词及清理停用词函数\n",
    "# 熟悉Python的可以使用 open('stopWord.txt').readlines（） 获取停用词list，效率更高\n",
    "stoplist = list(pd.read_csv('停用词.txt', names = ['w'], sep = 'aaa', \n",
    "                            encoding = 'utf-8', engine='python').w)\n",
    "import jieba \n",
    "def m_cut(intxt):\n",
    "    return [ w for w in jieba.cut(intxt) \n",
    "            if w not in stoplist and len(w) > 1 ] \n",
    "\n",
    "# 设定数据框转换函数\n",
    "def m_appdf(chapnum):\n",
    "    tmpdf = pd.DataFrame(m_cut(chapter.txt[chapnum + 1]), columns = ['word'])\n",
    "    tmpdf['chap'] = chapter.index[chapnum] # 也可以直接 = chapnum + 1\n",
    "    return tmpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部读入并转换为数据框\n",
    "\n",
    "df0 = pd.DataFrame(columns = ['word', 'chap']) # 初始化结果数据框\n",
    "\n",
    "for chapidx in range(len(chapter)):\n",
    "    df0 = df0.append(m_appdf(chapidx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出为序列格式\n",
    "df0.groupby(['word', 'chap']).agg('size').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接输出为数据框\n",
    "t2d = pd.crosstab(df0.word, df0.chap)\n",
    "len(t2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算各词条的总出现频次，准备进行低频词删减，axis=1表示按行统计\n",
    "totnum = t2d.agg(func = 'sum', axis=1)\n",
    "totnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#按行选取\n",
    "t2dclean = t2d.iloc[list(totnum >= 10),:]\n",
    "#求转置\n",
    "t2dclean.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用sklearn库实现\n",
    "***CountVectorizer类的基本用法***\n",
    "\n",
    "文本信息在向量化之前很难直接纳入建模分析，考虑到这一问题，专门用于数据挖掘的sklearn库提供了一个从文本信息到数据挖掘模型之间的桥梁，即CountVectorizer类，通过这一类中的功能，可以很容易地实现文档信息的向量化。\n",
    "\n",
    "> class sklearn.feature_extraction.text.CountVectorizer(\n",
    ">\n",
    "> input = 'content' : {'filename', 'file', 'content'}\n",
    ">\n",
    ">    #filename为所需读入的文件列表, file则为具体的文件名称。\n",
    ">\n",
    "> encoding='utf-8' #文档编码\n",
    ">\n",
    "> stop_words = None  #停用词列表，当analyzer == 'word'时才生效\n",
    ">\n",
    "> min_df / max_df : float in range [0.0, 1.0] or int, default = 1 / 1.0\n",
    ">\n",
    ">    #词频绝对值/比例的阈值，在此范围之外的将被剔除\n",
    ">    #小数格式说明提供的是百分比，如0.05指的就是5%的阈值\n",
    ">\n",
    ">）\n",
    ">\n",
    ">CountVectorizer.build_analyzer()\n",
    ">\n",
    ">#返回文本预处理和分词的可调用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer(min_df = 2) # 在两个以上文档中出现的才保留\n",
    "\n",
    "analyze = countvec.build_analyzer()\n",
    "analyze('郭靖 和 哀牢山 三十六 剑 。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec.fit(['郭靖 和 黄蓉 哀牢山 三十六 剑 。', '黄蓉 和 郭靖 郭靖'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec.get_feature_names() # 词汇列表，实际上就是获取每个列对应的词条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec.vocabulary_ # 词条字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = countvec.transform(['郭靖 和 黄蓉 哀牢山 三十六 剑 。', '黄蓉 和 郭靖 郭靖'])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.todense() # 将稀疏矩阵直接转换为标准格式矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec.fit_transform(['郭靖 和 哀牢山 三十六 剑 。', '黄蓉 和 郭靖 郭靖']) # 一次搞定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***使用sklearn生成射雕的章节d2m矩阵***\n",
    "\n",
    "将章节文档数据框处理为空格分隔词条的文本格式\n",
    "\n",
    "使用fit_transform函数生成bow稀疏矩阵\n",
    "\n",
    "转换为标准格式的d2m矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawchap = [ \" \".join(m_cut(w)) for w in chapter.txt.iloc[:5]] \n",
    "rawchap[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer(min_df = 5) # 在5个以上章节中出现的才保留\n",
    "\n",
    "res = countvec.fit_transform(rawchap)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从词袋模型到Bi-gram\n",
    "\n",
    "* 词袋模型完全无法利用语序信息\n",
    "\n",
    "  * 我帮你 vs 你帮我\n",
    "  \n",
    "  * P(我帮你) = P(我)\\*P(帮)\\*P(你)\n",
    "\n",
    "* Bi-gram:进一步保留顺序信息，两个词一起看\n",
    "\n",
    "  * P(我帮你) = P(我)\\*P(帮|我)\\*P(你|帮)\n",
    "  \n",
    "  * {\"我帮\":1, \"帮你\":2,\"你帮\":3,\"帮我\":4}\n",
    "  \n",
    "  * 我帮你 -> [1,1,0,0]\n",
    "  \n",
    "  * 你帮我 -> [0,0,1,1]\n",
    "\n",
    "* 显然，Bi-gram可以保留更多的文本有效信息。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从Bi-gram到N-gram\n",
    "\n",
    "* 考虑更多的前后词\n",
    "\n",
    "  * 可以直接扩展至tri-gram，4-gram直至N-gram\n",
    "  \n",
    "* 优点：考虑了词的顺序，信息量更充分\n",
    "\n",
    "  * 长度达到5之后，效果有明显提升\n",
    "  \n",
    "* 缺点：\n",
    "\n",
    "  * 词表迅速膨胀，数据出现大量的稀疏化问题\n",
    "  \n",
    "  * 没增加一个词，模型参数增加40万倍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 离散表示方式所面临的问题总结\n",
    "\n",
    "* 无法衡量词向量之间的关系\n",
    "  * 老公       [0,1,0,0,0,0,0,0,0,0]\n",
    "  \n",
    "  * 丈夫       [0,0,0,0,1,0,0,0,0,0]\n",
    "  \n",
    "  * 当家的      [0,0,0,0,0,0,0,1,0,0]\n",
    "  \n",
    "  * 挨千刀的    [0,0,0,0,0,0,0,0,1,0]\n",
    "  \n",
    "  * 各种度量（与或费、距离）都不合适，只能靠字典进行补充\n",
    "\n",
    "* 词表维度随着语料库增长膨胀\n",
    "\n",
    "* N-gram词序列随语料库膨胀更快\n",
    "\n",
    "* 数据稀疏问题（导致分析性能成为严重瓶颈）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档信息的分布式表示\n",
    "### 分布式表示(Distributed representation)\n",
    "\n",
    "* 如何定位围棋棋盘上的落子位置？\n",
    "\n",
    "  * 方法一：每个点单独记忆，共361个记忆单元\n",
    "  \n",
    "  * 方法二：行坐标+列坐标，共19+19=38个记忆单元\n",
    "  \n",
    "* 将分布式表示用于NLP\n",
    "\n",
    "  * 不直接考虑词与词在原文中的相对位置、距离、语法结构等，先把每个词看作一个单独向量\n",
    "  \n",
    "  * 根据一个词在上下文中的临近词的含义，应当可以归纳出词本身的含义\n",
    "  \n",
    "  * 单个词的词向量不足以表示整个文本，能表示的仅仅只是这个词本身\n",
    "  \n",
    "  * 事先决定用多少维度的向量来表示这个词条\n",
    "  \n",
    "    * 维度以50维和100维比较常见\n",
    "    \n",
    "    * 向量中每个维度的取值由模型训练决定，且不再是唯一的\n",
    "    \n",
    "      * [0.762, 0.107, 0.307, -0.199, 0.521,...]\n",
    "   \n",
    "   * 所有的词都在同一个高维空间中构成不同的向量\n",
    "   \n",
    "     * 从而词与词之间的关系就可以用空间中的距离来加以表述\n",
    "     \n",
    "   * 所有训练方法都是在训练语言模型的同时，顺便得到词向量的\n",
    "   \n",
    "     * 语言模型其实就是看一句话是不是正常人说出来的，具体表现为词条先后出现的顺序和距离所对应的概率是否最大化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 共现矩阵 (Cocurrence matrix)\n",
    "\n",
    "* 例如：语料库如下：\n",
    "\n",
    "  * I like deep learning.\n",
    "  \n",
    "  * I like NLP.\n",
    "\n",
    "  * I enjoy flying.\n",
    "\n",
    "* 确定取词长度：\n",
    "\n",
    "  * 取词长度为1的结果\n",
    "  \n",
    "* 窗口长度越长，则信息量越丰富，但数据量也越大\n",
    "\n",
    "  * 一般设为5--10\n",
    "  \n",
    "* 共现矩阵的行/列数值自然就表示出各个词汇的相似度\n",
    " \n",
    "  * 从而可以用作分析向量\n",
    "  \n",
    "则共现矩阵表示如下：\n",
    "![](./concurrence_matrix.jpg)\n",
    "\n",
    "例如：“I like”出现在第1，2句话中，一共出现2次，所以=2。\n",
    "对称的窗口指的是，“like I”也是2次\n",
    "\n",
    "**将共现矩阵行(列)作为词向量表示后，可以知道like，enjoy都是在I附近且统计数目大约相等，他们意思相近**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 仍然存在的问题\n",
    "\n",
    "* 如果将共现矩阵（列）直接作为词向量\n",
    "\n",
    "  * 向量维数随着词典大小线性增长\n",
    "  \n",
    "  * 存储整个词典的空间消耗非常大\n",
    "\n",
    "  * 一些模型如文本分类模型会面临稀疏性问题\n",
    "  \n",
    "  * 高度的稀疏性导致模型会欠稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战4：生成词向量\n",
    "尝试编制以下程序：\n",
    "\n",
    "    以段为单位依次读入射雕第一回的内容。\n",
    "    为每一段分别生成bow稀疏向量。\n",
    "    生成稀疏向量的同时动态更新字典。\n",
    "\n",
    "请自行编制bow稀疏向量和标准长向量互相转换的程序。\n",
    "\n",
    "在文档词条矩阵中可以看到许多类似“黄蓉道”、“黄蓉说”之类的词条，请思考对此有哪些处理办法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关键词提取\n",
    "\n",
    "### 用途\n",
    "\n",
    "* 用核心信息代表原始文档\n",
    "\n",
    "* 在文本聚类、分类、自动摘要等领域中有着重要应用\n",
    "\n",
    "### 需求：针对一篇文章，在不加人工干预的情况下提取关键词\n",
    "### 当然，首先要进行分词\n",
    "### 关键词分配： 事先给定关键词库，然后在文档中进行关键词检索\n",
    "### 关键词提取：根据某种规则，从文档中抽取最重要的词作为关键词\n",
    "* 有监督：抽取出候选词并标记是否为关键词，然后训练相应的模型\n",
    "\n",
    "* 无监督：给词条打分，并基于最高分值抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 无监督方式的分析思路——基于词频\n",
    "\n",
    "### 分析思路1：按照词频高低进行提取\n",
    "\n",
    "* 大量的高频词并无多少意义（停用词）\n",
    "\n",
    "* 即使出现频率相同，常见词价值也明显低于不常见词\n",
    "\n",
    "### 分析思路2：按照词条在文档中的重要性进行提取\n",
    "\n",
    "* 如何确定词条在该文档中的重要性？\n",
    "\n",
    "常见的方法：**TF-IDF、网络图**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF算法\n",
    "\n",
    "### 信息检索（IR）中最常用的一种文本关键信息表示法\n",
    "\n",
    "### 基本思想：\n",
    "  \n",
    "* 如果某个词在一篇文档中出现频率较高，并且在语料库中其他文本中出现频率较低，甚至不出现，则认为这个词具有很好的类别区分能力\n",
    "\n",
    "### 词频TF：Term Frequency，衡量一个词在文档中出现的频率\n",
    "\n",
    "* 平均而言出现越频繁的词，其重要性可能就越高\n",
    "\n",
    "#### 考虑到文章长度的差异，需要对词频做标准化\n",
    "\n",
    "* TF(w) = (w出现在文档中的次数)/(文档中的词的总数)\n",
    "\n",
    "* TF(w) = (w出现在文档中的次数)/(文档中出现最多的词的次数)\n",
    "\n",
    "### 逆文档频率IDF：Inverse Document Frequency，用于模拟在该语料库中，某一个词有多重要\n",
    "\n",
    "* 有些词到处出现，但是明显是没有用的。比如各种停用词，过渡句用词等。\n",
    "\n",
    "* 因此把罕见的词的重要性（weight）调高，把常见词的重要性调低\n",
    "\n",
    "### IDF的具体算法\n",
    "\n",
    "* IDF(w) = log(语料库中的文档总数/(含有该w的文档总数+1))\n",
    "\n",
    "### TF-IDF = TF * IDF\n",
    "\n",
    "* TF-IDF与一个词在文档中的出现次数成正比\n",
    "\n",
    "* 与该词在整个语料中的出现次数成反比\n",
    "\n",
    "### 优点\n",
    "* 简单快速\n",
    "\n",
    "* 结果也比较符合实际情况\n",
    "\n",
    "### 缺点\n",
    "* 单纯以“词频”横量一个词的重要性，不够全面，有时重要的词可能出现的次数并不多\n",
    "\n",
    "* 无法考虑词与词之间的相互关系\n",
    "\n",
    "* 这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的\n",
    "\n",
    "  * 一种解决方式是，对全文的第一段和每一段的第一句话，给予较大的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF的具体实现\n",
    "\n",
    "jieba, NLTK, sklearn, gensim等程序包都可以实现TF-IDF的计算。除算法细节上会有差异外，更多的是数据输入/输出格式上的不同。\n",
    "\n",
    "### jieba\n",
    "\n",
    "输出结果会自动按照TF-IDF值降序排列，并且直接给出的是词条而不是字典ID，便于阅读使用。\n",
    "\n",
    "可在计算TF-IDF时直接完成分词，并使用停用词表和自定义词库，非常方便。\n",
    "\n",
    "有默认的IDF语料库，可以不训练模型，直接进行计算。\n",
    "\n",
    "以单个文本为单位进行分析。\n",
    "\n",
    "> jieba.analyse.extract_tags(\n",
    "> \n",
    "> sentence 为待提取的文本\n",
    "> \n",
    "> topK = 20 : 返回几个 TF/IDF 权重最大的关键词\n",
    ">\n",
    "> withWeight = False : 是否一并返回关键词权重值\n",
    ">\n",
    ">allowPOS = () : 仅包括指定词性的词，默认值为空，即不筛选\n",
    ">\n",
    ">)\n",
    "\n",
    "**jieba.analyse.set_idf_path(file_name)**\n",
    "\n",
    "    关键词提取时使用自定义逆向文件频率（IDF）语料库 \n",
    "\n",
    "> 劳动防护 13.900677652\n",
    ">\n",
    "> 生化学 13.900677652\n",
    ">\n",
    "> 奥萨贝尔 13.900677652\n",
    "> \n",
    "> 奧薩貝爾 13.900677652\n",
    "> \n",
    "> 考察队员 13.900677652\n",
    "\n",
    "**jieba.analyse.set_stop_words(file_name)**\n",
    "\n",
    "    关键词提取时使用自定义停止词（Stop Words）语料库 \n",
    "\n",
    "**jieba.analyse.TFIDF(idf_path = None)**\n",
    "\n",
    "    新建 TFIDF模型实例\n",
    "    idf_path : 读取已有的TFIDF频率文件（即已有模型）\n",
    "    使用该实例提取关键词：TFIDF实例.extract_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chap</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>全本全集精校小说尽在：http://www.yimuhe.com/u/anglewing26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>第一回 风雪惊变钱塘江浩浩江水，日日夜夜无穷无休的从两浙西路临安府牛家村边绕过，东流入海。江...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2.0</td>\n",
       "      <td>第二回 江南七怪颜烈跨出房门，过道中一个中年士人拖着鞋皮，踢跶踢跶的直响，一路打着哈欠迎面过...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3.0</td>\n",
       "      <td>第三回 黄沙莽莽寺里僧众见焦木圆寂，尽皆悲哭。有的便为伤者包扎伤处，抬入客舍。忽听得巨钟下的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4.0</td>\n",
       "      <td>第四回 黑风双煞完颜洪熙笑道：“好，再打他个痛快。”蒙古兵前哨报来：“王罕亲自前来迎接大金国...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5.0</td>\n",
       "      <td>第五回 弯弓射雕一行人下得山来，走不多时，忽听前面猛兽大吼声一阵阵传来。韩宝驹一提缰，胯下黄...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6.0</td>\n",
       "      <td>第六回 崖顶疑阵午饭以后，郭靖来到师父帐中。全金发道：“靖儿，我试试你的开山掌练得怎样了。”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7.0</td>\n",
       "      <td>第七回 比武招亲江南六怪与郭靖晓行夜宿，向东南进发，在路非止一日，过了大漠草原。这天离张家口...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8.0</td>\n",
       "      <td>第八回 各显神通王处一脚步好快，不多时便带同郭靖到了城外，再行数里，到了一个山峰背后。他不住...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9.0</td>\n",
       "      <td>第九回 铁枪破犁郭黄二人来到赵王府后院，越墙而进，黄蓉柔声道：“你轻身功夫好得很啊！”郭靖伏...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10.0</td>\n",
       "      <td>第十回 往事如烟完颜康陡然见到杨铁心，惊诧之下，便即认出，大叫：“啊，是你！”提起铁枪，“行...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11.0</td>\n",
       "      <td>第十一回 长春服输沙通天见师弟危殆，跃起急格，挡开了梅超风这一抓，两人手腕相交，都感臂酸心惊...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12.0</td>\n",
       "      <td>第十二回 亢龙有悔黄蓉正要将鸡撕开，身后忽然有人说道：“撕作三份，鸡屁股给我。”两人都吃了一...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13.0</td>\n",
       "      <td>第十三回 五湖废人黄蓉回到客店安睡，自觉做了一件好事，大为得意，一宵甜睡，次晨对郭靖说了。郭...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14.0</td>\n",
       "      <td>第十四回 桃花岛主五男一女，走进厅来，却是江南六怪。他们自北南来，离故乡日近，这天经过太湖，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15.0</td>\n",
       "      <td>第十五回 神龙摆尾陆冠英扶起完颜康，见他给点中了穴道，动弹不得，只两颗眼珠光溜溜地转动。陆乘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16.0</td>\n",
       "      <td>第十六回 《九阴真经》郭黄二人自程府出来，累了半夜，正想回客店安歇，忽听马蹄声响，一骑马自南...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17.0</td>\n",
       "      <td>第十七回 双手互搏周伯通道：“你道是我师哥死后显灵？还是还魂复生？都不是，他是假死。”郭靖“...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18.0</td>\n",
       "      <td>第十八回 三道试题郭靖循着蛇声走去，走出数十步，月光下果见数千条青蛇排成长队蜿蜒而前。十多名...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19.0</td>\n",
       "      <td>第十九回 洪涛群鲨洪七公万想不到这场背书比赛竟会如此收场，较之郭靖将欧阳克连摔十七八个筋斗都...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20.0</td>\n",
       "      <td>第二十回 九阴假经洪七公与郭靖见欧阳锋叔侄领周伯通走入后舱，径行到前舱换衣。四名白衣少女过来...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21.0</td>\n",
       "      <td>第二十一回 千钧巨岩欧阳锋只感身上炙热，脚下船板震动甚剧，知道这截船身转眼就要沉没，但洪七公...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22.0</td>\n",
       "      <td>第二十二回 骑鲨遨游黄蓉见欧阳锋拖泥带水地将侄儿抱上岸来，他向来阴鸷的脸上竟也笑逐颜开，可是...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23.0</td>\n",
       "      <td>第二十三回 大闹禁宫黄药师满腔悲愤，指天骂地，咒鬼斥神，痛责命运对他不公，命舟子将船驶往大陆...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24.0</td>\n",
       "      <td>第二十四回 密室疗伤黄蓉向外走了两步，回过头来，见郭靖眼光中露出怀疑神色，料想是自己脸上的杀...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25.0</td>\n",
       "      <td>第二十五回 荒村野店黄药师仰天一笑，说道：“冠英和这位姑娘留着。”陆冠英早知是祖师爷到了，但...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26.0</td>\n",
       "      <td>第二十六回 新盟旧约黄药师心想不明不白地跟全真七子大战一场，更不明不白地结下了深仇，真是好没...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27.0</td>\n",
       "      <td>第二十七回 轩辕台前两人正闹间，楼梯声响，适才随杨康下去的丐帮三长老又回了上来，走到郭黄二人...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28.0</td>\n",
       "      <td>第二十八回 铁掌峰顶此时鲁有脚已经醒转，四长老聚在一起商议。鲁有脚道：“现下真相未明，咱们须...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29.0</td>\n",
       "      <td>第二十九回 黑沼隐女郭靖在雕背连声呼叫，召唤小红马在地下跟来。转眼之间，双雕已飞出老远。雌雄...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30.0</td>\n",
       "      <td>第三十回 一灯大师两人顺着山路向前走去，行不多时，山路就到了尽头，前面是条宽约尺许的石梁，横...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31.0</td>\n",
       "      <td>第三十一回 鸳鸯锦帕一灯大师低低叹了口气道：“其实真正的祸根，还在我自己。我乃大理国小君，虽...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32.0</td>\n",
       "      <td>第三十二回 湍江险滩穆念慈右手让黄蓉握着，望着水面的落花，说道：“我见他杀了欧阳克，只道他从...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33.0</td>\n",
       "      <td>第三十三回 来日大难郭靖与黄蓉此刻心意欢畅，原不想理会闲事，但听到“老顽童”三字，心中一凛，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34.0</td>\n",
       "      <td>第三十四回 岛上巨变郭靖低声道：“蓉儿，你还要什么？”黄蓉道：“我还要什么？什么也不要啦！”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35.0</td>\n",
       "      <td>第三十五回 铁枪庙中船靠岸边，走上二三十人来，彭连虎、沙通天等人均在其内。最后上岸的一高一矮...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36.0</td>\n",
       "      <td>第三十六回 大军西征黄蓉幽幽地道：“欧阳伯伯赞得我可太好了。现下郭靖中你之计，和我爹爹势不两...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37.0</td>\n",
       "      <td>第三十七回 从天而降这一日郭靖驻军那密河畔，晚间正在帐中研读兵书，忽听帐外喀的一声轻响。帐门...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38.0</td>\n",
       "      <td>第三十八回 锦囊密令郭靖陪了丘处机与他门下十八名弟子李志常、尹志平、夏志诚、于志可，张志素、...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39.0</td>\n",
       "      <td>第三十九回 是非善恶郭靖纵马急驰数日，已离险地。缓缓南归，天时日暖，青草日长，沿途兵革之余，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40.0</td>\n",
       "      <td>第四十回 华山论剑欧阳锋冷冷地道：“早到早比，迟到迟比。老叫化，你今日跟我是比武决胜呢，还是...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    txt\n",
       "chap                                                   \n",
       "0.0   全本全集精校小说尽在：http://www.yimuhe.com/u/anglewing26...\n",
       "1.0   第一回 风雪惊变钱塘江浩浩江水，日日夜夜无穷无休的从两浙西路临安府牛家村边绕过，东流入海。江...\n",
       "2.0   第二回 江南七怪颜烈跨出房门，过道中一个中年士人拖着鞋皮，踢跶踢跶的直响，一路打着哈欠迎面过...\n",
       "3.0   第三回 黄沙莽莽寺里僧众见焦木圆寂，尽皆悲哭。有的便为伤者包扎伤处，抬入客舍。忽听得巨钟下的...\n",
       "4.0   第四回 黑风双煞完颜洪熙笑道：“好，再打他个痛快。”蒙古兵前哨报来：“王罕亲自前来迎接大金国...\n",
       "5.0   第五回 弯弓射雕一行人下得山来，走不多时，忽听前面猛兽大吼声一阵阵传来。韩宝驹一提缰，胯下黄...\n",
       "6.0   第六回 崖顶疑阵午饭以后，郭靖来到师父帐中。全金发道：“靖儿，我试试你的开山掌练得怎样了。”...\n",
       "7.0   第七回 比武招亲江南六怪与郭靖晓行夜宿，向东南进发，在路非止一日，过了大漠草原。这天离张家口...\n",
       "8.0   第八回 各显神通王处一脚步好快，不多时便带同郭靖到了城外，再行数里，到了一个山峰背后。他不住...\n",
       "9.0   第九回 铁枪破犁郭黄二人来到赵王府后院，越墙而进，黄蓉柔声道：“你轻身功夫好得很啊！”郭靖伏...\n",
       "10.0  第十回 往事如烟完颜康陡然见到杨铁心，惊诧之下，便即认出，大叫：“啊，是你！”提起铁枪，“行...\n",
       "11.0  第十一回 长春服输沙通天见师弟危殆，跃起急格，挡开了梅超风这一抓，两人手腕相交，都感臂酸心惊...\n",
       "12.0  第十二回 亢龙有悔黄蓉正要将鸡撕开，身后忽然有人说道：“撕作三份，鸡屁股给我。”两人都吃了一...\n",
       "13.0  第十三回 五湖废人黄蓉回到客店安睡，自觉做了一件好事，大为得意，一宵甜睡，次晨对郭靖说了。郭...\n",
       "14.0  第十四回 桃花岛主五男一女，走进厅来，却是江南六怪。他们自北南来，离故乡日近，这天经过太湖，...\n",
       "15.0  第十五回 神龙摆尾陆冠英扶起完颜康，见他给点中了穴道，动弹不得，只两颗眼珠光溜溜地转动。陆乘...\n",
       "16.0  第十六回 《九阴真经》郭黄二人自程府出来，累了半夜，正想回客店安歇，忽听马蹄声响，一骑马自南...\n",
       "17.0  第十七回 双手互搏周伯通道：“你道是我师哥死后显灵？还是还魂复生？都不是，他是假死。”郭靖“...\n",
       "18.0  第十八回 三道试题郭靖循着蛇声走去，走出数十步，月光下果见数千条青蛇排成长队蜿蜒而前。十多名...\n",
       "19.0  第十九回 洪涛群鲨洪七公万想不到这场背书比赛竟会如此收场，较之郭靖将欧阳克连摔十七八个筋斗都...\n",
       "20.0  第二十回 九阴假经洪七公与郭靖见欧阳锋叔侄领周伯通走入后舱，径行到前舱换衣。四名白衣少女过来...\n",
       "21.0  第二十一回 千钧巨岩欧阳锋只感身上炙热，脚下船板震动甚剧，知道这截船身转眼就要沉没，但洪七公...\n",
       "22.0  第二十二回 骑鲨遨游黄蓉见欧阳锋拖泥带水地将侄儿抱上岸来，他向来阴鸷的脸上竟也笑逐颜开，可是...\n",
       "23.0  第二十三回 大闹禁宫黄药师满腔悲愤，指天骂地，咒鬼斥神，痛责命运对他不公，命舟子将船驶往大陆...\n",
       "24.0  第二十四回 密室疗伤黄蓉向外走了两步，回过头来，见郭靖眼光中露出怀疑神色，料想是自己脸上的杀...\n",
       "25.0  第二十五回 荒村野店黄药师仰天一笑，说道：“冠英和这位姑娘留着。”陆冠英早知是祖师爷到了，但...\n",
       "26.0  第二十六回 新盟旧约黄药师心想不明不白地跟全真七子大战一场，更不明不白地结下了深仇，真是好没...\n",
       "27.0  第二十七回 轩辕台前两人正闹间，楼梯声响，适才随杨康下去的丐帮三长老又回了上来，走到郭黄二人...\n",
       "28.0  第二十八回 铁掌峰顶此时鲁有脚已经醒转，四长老聚在一起商议。鲁有脚道：“现下真相未明，咱们须...\n",
       "29.0  第二十九回 黑沼隐女郭靖在雕背连声呼叫，召唤小红马在地下跟来。转眼之间，双雕已飞出老远。雌雄...\n",
       "30.0  第三十回 一灯大师两人顺着山路向前走去，行不多时，山路就到了尽头，前面是条宽约尺许的石梁，横...\n",
       "31.0  第三十一回 鸳鸯锦帕一灯大师低低叹了口气道：“其实真正的祸根，还在我自己。我乃大理国小君，虽...\n",
       "32.0  第三十二回 湍江险滩穆念慈右手让黄蓉握着，望着水面的落花，说道：“我见他杀了欧阳克，只道他从...\n",
       "33.0  第三十三回 来日大难郭靖与黄蓉此刻心意欢畅，原不想理会闲事，但听到“老顽童”三字，心中一凛，...\n",
       "34.0  第三十四回 岛上巨变郭靖低声道：“蓉儿，你还要什么？”黄蓉道：“我还要什么？什么也不要啦！”...\n",
       "35.0  第三十五回 铁枪庙中船靠岸边，走上二三十人来，彭连虎、沙通天等人均在其内。最后上岸的一高一矮...\n",
       "36.0  第三十六回 大军西征黄蓉幽幽地道：“欧阳伯伯赞得我可太好了。现下郭靖中你之计，和我爹爹势不两...\n",
       "37.0  第三十七回 从天而降这一日郭靖驻军那密河畔，晚间正在帐中研读兵书，忽听帐外喀的一声轻响。帐门...\n",
       "38.0  第三十八回 锦囊密令郭靖陪了丘处机与他门下十八名弟子李志常、尹志平、夏志诚、于志可，张志素、...\n",
       "39.0  第三十九回 是非善恶郭靖纵马急驰数日，已离险地。缓缓南归，天时日暖，青草日长，沿途兵革之余，...\n",
       "40.0  第四十回 华山论剑欧阳锋冷冷地道：“早到早比，迟到迟比。老叫化，你今日跟我是比武决胜呢，还是..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 载入语料\n",
    "raw = pd.read_csv(\"金庸-射雕英雄传txt精校版.txt\",\n",
    "                  names = ['txt'], sep ='aaa', encoding =\"utf-8\" ,engine='python')\n",
    "\n",
    "# 章节判断用变量预处理\n",
    "def m_head(tmpstr):\n",
    "    return tmpstr[:1]\n",
    "\n",
    "def m_mid(tmpstr):\n",
    "    return tmpstr.find(\"回 \")\n",
    "\n",
    "raw['head'] = raw.txt.apply(m_head)\n",
    "raw['mid'] = raw.txt.apply(m_mid)\n",
    "raw['len'] = raw.txt.apply(len)\n",
    "# 章节判断\n",
    "chapnum = 0\n",
    "for i in range(len(raw)):\n",
    "    if raw['head'][i] == \"第\" and raw['mid'][i] > 0 and raw['len'][i] < 30 :\n",
    "        chapnum += 1\n",
    "    if chapnum >= 40 and raw['txt'][i] == \"附录一：成吉思汗家族\" :\n",
    "        chapnum = 0\n",
    "    raw.loc[i, 'chap'] = chapnum\n",
    "    \n",
    "# 删除临时变量\n",
    "del raw['head']\n",
    "del raw['mid']\n",
    "del raw['len']\n",
    "\n",
    "rawgrp = raw.groupby('chap')\n",
    "chapter = rawgrp.agg(sum) # 只有字符串列的情况下，sum函数自动转为合并字符串\n",
    "#chapter = chapter[chapter.index != 0]\n",
    "\n",
    "chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.908 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['杨铁心',\n",
       " '包惜弱',\n",
       " '郭啸天',\n",
       " '颜烈',\n",
       " '丘处机',\n",
       " '武官',\n",
       " '杨二人',\n",
       " '官兵',\n",
       " '曲三',\n",
       " '金兵',\n",
       " '那道人',\n",
       " '道长',\n",
       " '娘子',\n",
       " '段天德',\n",
       " '咱们',\n",
       " '临安',\n",
       " '说道',\n",
       " '丈夫',\n",
       " '杨家枪',\n",
       " '两人']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "# 注意：函数是在使用默认的TFIDF模型进行分析！\n",
    "jieba.analyse.extract_tags(chapter.txt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('杨铁心', 0.21886511509515091),\n",
       " ('包惜弱', 0.1685852913570757),\n",
       " ('郭啸天', 0.09908082913091291),\n",
       " ('颜烈', 0.05471627877378773),\n",
       " ('丘处机', 0.049556061537506184),\n",
       " ('武官', 0.04608486747703612),\n",
       " ('杨二人', 0.044305304110440376),\n",
       " ('官兵', 0.040144546232276104),\n",
       " ('曲三', 0.03439059290450272),\n",
       " ('金兵', 0.0336976598949901),\n",
       " ('那道人', 0.03117114380098961),\n",
       " ('道长', 0.02912588670625928),\n",
       " ('娘子', 0.026796070076125684),\n",
       " ('段天德', 0.025139911869037603),\n",
       " ('咱们', 0.023296768210644483),\n",
       " ('临安', 0.022991990912831523),\n",
       " ('说道', 0.022350916333591046),\n",
       " ('丈夫', 0.02221595763081643),\n",
       " ('杨家枪', 0.019765724469755074),\n",
       " ('两人', 0.0192267944114003)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.analyse.extract_tags(chapter.txt[1], withWeight = True) # 要求返回权重值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('杨铁心', 0.24787133516800222),\n",
       " ('包惜弱', 0.1909279203321098),\n",
       " ('郭啸天', 0.11221202335308209),\n",
       " ('曲三', 0.06426483083720931),\n",
       " ('颜烈', 0.061967833792000555),\n",
       " ('丘处机', 0.056123732343681704),\n",
       " ('武官', 0.052192500516161394),\n",
       " ('杨二人', 0.050177091402185486),\n",
       " ('官兵', 0.04546490778113197),\n",
       " ('金兵', 0.038163614820832165)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 应用自定义词典改善分词效果\n",
    "jieba.load_userdict('金庸小说词库.txt') # dict为自定义词典的路径\n",
    "\n",
    "# 在TFIDF计算中直接应用停用词表\n",
    "jieba.analyse.set_stop_words('停用词.txt')\n",
    "\n",
    "TFres = jieba.analyse.extract_tags(chapter.txt[1], withWeight = True)\n",
    "TFres[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('杨铁心', 0.24787133516800222),\n",
       " ('包惜弱', 0.1909279203321098),\n",
       " ('郭啸天', 0.11221202335308209),\n",
       " ('武官', 0.07034186538551414),\n",
       " ('颜烈', 0.061967833792000555),\n",
       " ('说道', 0.05861822115459512),\n",
       " ('丘处机', 0.056123732343681704),\n",
       " ('曲三', 0.055268608517189684),\n",
       " ('一个', 0.053593802198486966),\n",
       " ('杨二人', 0.053593802198486966)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用自定义TF-IDF频率文件\n",
    "jieba.analyse.set_idf_path(\"idf.txt.big\")\n",
    "TFres1 = jieba.analyse.extract_tags(chapter.txt[1], withWeight = True)\n",
    "TFres1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn\n",
    "\n",
    "输出格式为矩阵，直接为后续的sklearn建模服务。\n",
    "\n",
    "需要先使用背景语料库进行模型训练。\n",
    "\n",
    "结果中给出的是字典ID而不是具体词条，直接阅读结果比较困难。\n",
    "\n",
    "class sklearn.feature_extraction.text.TfidfTransformer()\n",
    "\n",
    "发现参数基本上都不用动，所以这里就不介绍了.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "txtlist = [ \" \".join(m_cut(w)) for w in chapter.txt.iloc[:5]] \n",
    "\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(txtlist) # 将文本中的词语转换为词频矩阵  \n",
    "\n",
    "transformer = TfidfTransformer()  \n",
    "tfidf = transformer.fit_transform(X)  #基于词频矩阵X计算TF-IDF值  \n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.toarray() # 转换为数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.todense() # 转换为矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"字典长度：\", len(vectorizer.vocabulary_))\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim\n",
    "\n",
    "输出格式为list，目的也是为后续的建模分析服务。\n",
    "\n",
    "需要先使用背景语料库进行模型训练。\n",
    "\n",
    "结果中给出的是字典ID而不是具体词条，直接阅读结果比较困难。\n",
    "\n",
    "gensim也提供了sklearn的API接口：sklearn_api.tfidf，可以在sklearn中直接使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文档分词及预处理  \n",
    "chaplist = [m_cut(w) for w in chapter.txt.iloc[:5]]\n",
    "chaplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models  \n",
    "\n",
    "# 生成文档对应的字典和bow稀疏向量\n",
    "dictionary = corpora.Dictionary(chaplist)  \n",
    "corpus = [dictionary.doc2bow(text) for text in chaplist] # 仍为list in list  \n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = models.TfidfModel(corpus) # 建立TF-IDF模型  \n",
    "corpus_tfidf = tfidf_model[corpus] # 对所需文档计算TF-IDF结果\n",
    "corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf[3] # 列出所需文档的TF-IDF计算结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.token2id # 列出字典内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank算法\n",
    "\n",
    "TextRank算法的jieba实现\n",
    "> jieba.analyse.textrank(\n",
    ">\n",
    ">    sentence, topK=20, withWeight=False,\n",
    ">\n",
    ">    allowPOS=('ns', 'n', 'vn', 'v')\n",
    ">\n",
    ">) # 注意默认过滤词性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战练习\n",
    "\n",
    "请使用《射雕》全文计算出jieba分词的IDF语料库，然后使用该语料库重新对第一章计算关键词。比较这样的分析结果和以前有何不同。\n",
    "\n",
    "请自行编制将jieba分词的TF-IDF结果转换为文档-词条矩阵格式的程序。\n",
    "\n",
    "请自行思考本章提供的三种TF-IDF实现方式的使用场景是什么。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 抽取文档主题\n",
    "\n",
    "## 什么是主题模型\n",
    "\n",
    "### LDA，Latent Dirichlet Allocation\n",
    "\n",
    "* Q: 有这么一篇文章，里面提到了詹姆斯、湖人队、季后赛，请问这篇文章最可能的主题是什么？\n",
    "\n",
    "    * 军事\n",
    "    * 体育\n",
    "    * 养生\n",
    "    * 教育\n",
    "    \n",
    "### LDA由Blei于2003年提出，其基本思想是把文档看成各种隐含主题的混合，而每个主题则表现为跟该主题相关的词项的概率分布\n",
    "\n",
    "* 该方法不需要任何关于文本的背景知识\n",
    "* 隐含主题的引入使得分析者可以对“一词多义”和“一义多词”的语言现象进行建模，更接近人类语言交互的特征\n",
    "\n",
    "### LDA基于词袋模型构建，认为文档和单词都是可交换的，忽略单词在文档中的顺序和文档在语料库中的顺序，从而将文本信息转化为易于建模的数字信息\n",
    "\n",
    "* 主题就是一个桶，里面装了出现概率较高的单词，这些单词与这个主题有很强的的相关性\n",
    "\n",
    "## LDA模型包含词项、主题和文档三层结构\n",
    "\n",
    "### 本质上，LDA简单粗暴的认为：文章中的每个词都是通过“以一定概率选择某个主题，再从该主题中以一定概率选择某个词”得到的\n",
    "\n",
    "### 一个词可能会关联很多主题，因此需要计算各种情况下的概率分布，来确定最可能出现的主题是哪种\n",
    "\n",
    "* 体育：{姚明：0.3，篮球：0.5，拳击：0.2，李现：0.03，王宝强：0.03，杨紫：0.04}\n",
    "* 娱乐：{姚明：0.03，篮球：0.03，足球：0.04，李现：0.6，王宝强：0.7，杨紫：0.8}\n",
    "\n",
    "### 一篇文章可能会涉及到几个主题，因此也需要计算多个主题的概率\n",
    "\n",
    "* 体育新闻：\\[废话，体育，体育，体育，....，娱乐，娱乐\\]\n",
    "* 八卦消息：\\[废话，废话，废话，废话，....，娱乐，娱乐\\]\n",
    "\n",
    "## LDA中涉及到的数学知识\n",
    "\n",
    "### 多项式分布：主题和词汇的概率分布服从多项式分布\n",
    "\n",
    "* 如果1个词汇主题，就是大家熟悉的二项分布\n",
    "\n",
    "### Dirichlet分布：上述多项式分布的参数为随机变量，均服从Dirichlet分布\n",
    "\n",
    "### Gibbs抽样：直接求LDA的精确参数分布计算量太大，实际上不可行，因此通过Gibbs抽烟减小计算量，得到逼近的结果\n",
    "\n",
    "* 通过现有文章（已有主题，或者需要提取主题）训练处LDA模型\n",
    "* 用模型预测新的文章所属主题分类\n",
    "\n",
    "### 主题模型对于  ***短文本***  效果不好\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主题模型的sklearn实现\n",
    "\n",
    "在scikit-learn中，LDA主题模型的类被放置在sklearn.decomposition.LatentDirichletAllocation类中，其算法实现主要基于变分推断EM算法，而没有使用基于Gibbs采样的MCMC算法实现。\n",
    "\n",
    "注意由于LDA是基于词频统计的，因此理论上一般不宜用TF-IDF来做文档特征，但并非不能尝试。实际分析中也确实会见到此类操作。\n",
    "\n",
    "> class sklearn.decomposition.LatentDirichletAllocation(\n",
    "> \n",
    "> n_components = None : 隐含主题数K，需要设置的最重要参数。\n",
    "    K的设定范围和具体的研究背景有关。\n",
    "    K越大，需要的文档样本越多。\n",
    "> \n",
    "> doc_topic_prior = None : 文档主题先验Dirichlet分布的参数α，未设定则用1/K。\n",
    "> \n",
    "> topic_word_prior = None : 主题词先验Dirichlet分布的参数η，未设定则用1/K。\n",
    "> \n",
    "> learning_method = 'online' : 即LDA的求解算法。'batch' | 'online'\n",
    "    batch: 变分推断EM算法，会将将训练样本分批用于更新主题词分布，新版默认算法。\n",
    "        样本量不大只是用来学习的话用batch比较好，这样可以少很多参数要调。\n",
    "        需注意n_components(K), doc_topic_prior(α), topic_word_prior(η)\n",
    "    online: 在线变分推断EM算法，大样本时首选。\n",
    "        需进一步注意learning_decay, learning_offset，\n",
    "            total_samples和batch_size等参数。\n",
    "> \n",
    "> 仅在online算法时需要设定的参数\n",
    ">\n",
    "> learning_decay = 0.7 ：控制\"online\"算法的学习率，一般不用修改。\n",
    "        取值最好在(0.5, 1.0]，以保证\"online\"算法渐进的收敛。\n",
    ">\n",
    "> learning_offset = 10. ：用来减小前面训练样本批次对最终模型的影响。\n",
    "        取值要大于1。\n",
    ">\n",
    ">total_samples = 1e6 ： 分步训练时每一批文档样本的数量。\n",
    "         使用partial_fit进行模型拟合时才需要此参数。\n",
    ">\n",
    "> batch_size = 128 : 每次EM算法迭代时使用的文档样本的数量。\n",
    "> \n",
    "> )\n",
    "\n",
    "***将语料库转换为所需矩阵***\n",
    "\n",
    "除直接使用分词清理后文本进行转换外，也可以先计算关键词的TF-IDF值，然后使用关键词矩阵进行后续分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定分词及清理停用词函数\n",
    "# 熟悉Python的可以使用 open('stopWord.txt').readlines（） 获取停用词list，效率更高\n",
    "stoplist = list(pd.read_csv('停用词.txt', names = ['w'], sep = 'aaa', \n",
    "                            encoding = 'utf-8', engine='python').w)\n",
    "import jieba \n",
    "def m_cut(intxt):\n",
    "    return [ w for w in jieba.cut(intxt) \n",
    "            if w not in stoplist and len(w) > 1 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成分词清理后章节文本\n",
    "cleanchap = [ \" \".join(m_cut(w)) for w in chapter.txt] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文本中的词语转换为词频矩阵  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer(min_df = 5) \n",
    "\n",
    "wordmtx = countvec.fit_transform(cleanchap) \n",
    "wordmtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基于词频矩阵X计算TF-IDF值  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()  \n",
    "tfidf = transformer.fit_transform(wordmtx)  \n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定LDA模型\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 10\n",
    "ldamodel = LatentDirichletAllocation(n_components = n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拟合LDA模型\n",
    "ldamodel.fit(wordmtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拟合后模型的实质\n",
    "print(ldamodel.components_.shape)\n",
    "ldamodel.components_[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主题词打印函数\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] \n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 12\n",
    "tf_feature_names = countvec.get_feature_names()\n",
    "print_top_words(ldamodel, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim实现\n",
    "\n",
    "> class gensim.models.ldamodel.LdaModel(\n",
    ">\n",
    ">corpus = None : 用于训练模型的语料\n",
    ">\n",
    ">num_topics = 100 : 准备提取的主题数量\n",
    ">\n",
    ">id2word = None : 所使用的词条字典，便于结果阅读\n",
    ">\n",
    ">passes = 1 ：模型遍历语料库的次数，次数越多模型越精确，但是也更花时间\n",
    ">\n",
    ">)\n",
    "\n",
    "用新出现的语料更新模型\n",
    "\n",
    "    ldamodel.update(other_corpus)\n",
    "\n",
    "gensim也提供了sklearn的API接口：sklearn_api.ldamodel，可以在sklearn中直接使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定分词及清理停用词函数\n",
    "# 熟悉Python的可以使用 open('stopWord.txt').readlines（） 获取停用词list，效率更高\n",
    "stoplist = list(pd.read_csv('停用词.txt', names = ['w'], sep = 'aaa', \n",
    "                            encoding = 'utf-8', engine='python').w)\n",
    "import jieba \n",
    "def m_cut(intxt):\n",
    "    return [ w for w in jieba.cut(intxt) \n",
    "            if w not in stoplist and len(w) > 1 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文档预处理，提取主题词  \n",
    "chaplist = [m_cut(w) for w in chapter.txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成文档对应的字典和bow稀疏向量\n",
    "from gensim import corpora, models  \n",
    "\n",
    "dictionary = corpora.Dictionary(chaplist)  \n",
    "corpus = [dictionary.doc2bow(text) for text in chaplist] # 仍为list in list  \n",
    "\n",
    "tfidf_model = models.TfidfModel(corpus) # 建立TF-IDF模型  \n",
    "corpus_tfidf = tfidf_model[corpus] # 对所需文档计算TF-IDF结果\n",
    "corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# 列出所消耗的时间备查\n",
    "%time ldamodel = LdaModel(corpus, id2word = dictionary, \\\n",
    "                          num_topics = 10, passes = 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***列出最重要的前若干个主题***\n",
    "\n",
    "print_topics(num_topics=20, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算各语料的LDA模型值\n",
    "corpus_lda = ldamodel[corpus_tfidf] # 此处应当使用和模型训练时相同类型的矩阵\n",
    "\n",
    "for doc in corpus_lda:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检索和文本内容最接近的主题\n",
    "query = chapter.txt[1] # 检索和第1章最接近的主题\n",
    "query_bow = dictionary.doc2bow(m_cut(query)) # 频数向量\n",
    "query_tfidf = tfidf_model[query_bow] # TF-IDF向量\n",
    "print(\"转换后：\", query_tfidf[:10])\n",
    "\n",
    "ldamodel.get_document_topics(query_bow) # 需要输入和文档对应的bow向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检索和文本内容最接近的主题\n",
    "ldamodel[query_tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果的图形化呈现\n",
    "\n",
    "pyLDAvis包引入自R，可以用交互式图形的方式呈现主题模型的分析结果。\n",
    "\n",
    "同时支持sklearn和gensim包。\n",
    "\n",
    "在许多系统配置下都会出现兼容问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对sklearn的LDA结果作呈现\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.sklearn.prepare(ldamodel, tfidf, countvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.disable_notebook() # 关闭notebook支持后，可以看到背后所生成的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战练习\n",
    "\n",
    "在其余参数全部固定不变的情况下，尝试分别用清理前矩阵、清理后原始矩阵、TF-IDF矩阵进行LDA模型拟合，比较分析结果。\n",
    "\n",
    "在gensim拟合LDA时，分别将passes参数设置为1、5、10、50、100等，观察结果变化的情况，思考如何对该参数做最优设定。\n",
    "\n",
    "请尝试对模型进行优化，得到对本案例较好的分析结果。\n",
    "\n",
    "提示：使用gensim进行拟合更容易一些。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文档相似度\n",
    "\n",
    "## 用途\n",
    "   * 搜索引擎的类似文章推荐   \n",
    "   * 购物网站的类似商品推荐\n",
    "   * 点评网站/微博微信平台上的类似内容推荐\n",
    "\n",
    "## 基于词袋模型的基本思路\n",
    "   * 如果两个文档/两句话的用词越相似，他们的内容就应该越相似。因此，可以从词频入手，计算他们的相似程度\n",
    "   * 文档向量化之后，相似度的考察就可以直接转化为计算空间中的距离问题\n",
    "   * 缺陷： 不能考虑否定词的巨大作用，不能考虑词序的差异\n",
    "   \n",
    "### 在本质上，向量空间中文本相似度的计算和任何聚类方法所考虑的问题***没有区别***\n",
    "\n",
    "## 余弦相似度\n",
    "### 两个向量间的夹角能够很好的反映其相似度\n",
    "\n",
    "   * 但夹角大小使用不便，因此用夹角的余弦值作为相似度衡量指标\n",
    "   * 思考：为什么只考虑夹角，不考虑相对距离?\n",
    "   \n",
    "### 余弦值越接近1，夹角越接近0度，两个向量也就越相似\n",
    "### 可以证明余弦值的计算公式可以直接扩展到n维空间\n",
    "### 因此在由n维向量所构成的空间中，可以利用余弦值来计算文档的相似度\n",
    "\n",
    "## 相似度计算：基本分析思路\n",
    "### 语料分词、清理\n",
    "  * 原始语料分词\n",
    "  * 语料清理\n",
    "### 语料向量化\n",
    "  * 将语料转换为词频向量\n",
    "  * 为了避免文章长度的差异，长度悬殊时可以考虑使用相对词频\n",
    "### 计算相似度\n",
    "  * 计算两个向量的余弦相似度，值越大表示越相似\n",
    "### 仍然存在的问题\n",
    "  * 高频词不一定具有文档代表性，导致相似度计算结果变差\n",
    "\n",
    "## 相似度计算：基本分析思路\n",
    "### 语料分词、清理\n",
    "  * 原始语料分词\n",
    "  * 语料清理\n",
    "### 语料向量化\n",
    "  * 将语料转换为基于关键词的词频向量\n",
    "  * 为了避免文章长度的差异，长度悬殊时可以考虑使用相对词频\n",
    "### 使用TF-IDF算法，找出两篇文章的关键词\n",
    "  * 例如取前20个，或者前50个\n",
    "### 计算相似度\n",
    "  * 计算两个向量的余弦相似度，值越大表示越相似\n",
    "\n",
    "### 当向量表示概率分布式，其他相似度测量方法比余弦相似度更好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词条相似度：word2vec\n",
    "\n",
    "词袋模型不考虑词条之间的相关性，因此无法用于计算词条相似度。\n",
    "\n",
    "分布式表达会考虑词条的上下文关联，因此能够提取出词条上下文中的相关性信息，而词条之间的相似度就可以直接利用此类信息加以计算。\n",
    "\n",
    "目前主要使用gensim实现相应的算法。\n",
    "\n",
    "gensim也提供了sklearn的API接口：sklearn_api.w2vmodel，可以在sklearn中直接使用。\n",
    "\n",
    "设置word2vec模型\n",
    "> class gensim.models.word2vec.Word2Vec(\n",
    ">\n",
    "> sentences = None : 类似list of list的格式，对于特别大的文本，尽量考虑流式处理\n",
    ">\n",
    "> size = 100 : 词条向量的维度，数据量充足时，300/500的效果会更好\n",
    ">\n",
    "> window = 5 : 上下文窗口大小\n",
    ">\n",
    "> workers = 3 : 同时运行的线程数，多核系统可明显加速计算\n",
    ">\n",
    ">其余细节参数设定：\n",
    ">\n",
    ">    min_count = 5 : 低频词过滤阈值，低于该词频的不纳入模型\n",
    ">\n",
    ">    max_vocab_size = None : 每1千万词条需要1G内存，必要时设定该参数以节约内存\n",
    ">\n",
    ">    sample=0.001 : 负例采样的比例设定\n",
    ">\n",
    ">    negative=5 : 一般为5-20，设为0时不进行负例采样\n",
    ">\n",
    ">    iter = 5 : 模型在语料库上的迭代次数，该参数将被取消\n",
    ">\n",
    ">与神经网络模型有关的参数设定：\n",
    ">\n",
    ">    seed=1, alpha=0.025, min_alpha=0.0001, sg=0, hs=0\n",
    ">\n",
    ">)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词和预处理，生成list of list格式\n",
    "import jieba\n",
    "\n",
    "chapter['cut'] = chapter.txt.apply(jieba.lcut)\n",
    "chapter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化word2vec模型和词表\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "n_dim = 300 # 指定向量维度，大样本量时300~500较好\n",
    "\n",
    "w2vmodel = Word2Vec(size = n_dim, min_count = 10)\n",
    "w2vmodel.build_vocab(chapter.cut) # 生成词表\n",
    "w2vmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***对word2vec模型进行训练***\n",
    "\n",
    ">word2vecmodel.train(\n",
    ">\n",
    "> sentences : iterable of iterables格式，对于特别大量的文本，尽量考虑流式处理\n",
    ">\n",
    ">total_examples = None : 句子总数，int，可直接使用model.corpus_count指定\n",
    ">\n",
    ">total_words = None : 句中词条总数，int，该参数和total_examples至少要指定一个\n",
    ">\n",
    ">epochs = None : 模型迭代次数，需要指定\n",
    ">\n",
    ">其他带默认值的参数设定：\n",
    ">\n",
    ">   start_alpha=None, end_alpha=None, word_count=0, queue_factor=2,\n",
    ">\n",
    ">   report_delay=1.0, compute_loss=False, callbacks=()\n",
    ">\n",
    ">)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在评论训练集上建模（大数据集时可能会花费几分钟）\n",
    "# 本例消耗内存较少\n",
    "#time \n",
    "w2vmodel.train(chapter.cut, \\\n",
    "               total_examples = w2vmodel.corpus_count, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练完毕的模型实质\n",
    "print(w2vmodel.wv[\"郭靖\"].shape)\n",
    "w2vmodel.wv[\"郭靖\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w2v模型的保存和复用\n",
    "\n",
    "> w2vmodel.save(存盘路径及文件名称)\n",
    "> w2vmodel.load(存盘路径及文件名称)\n",
    "\n",
    "词向量间的相似度\n",
    "> w2vmodel.wv.most_similar(词条)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel.wv.most_similar(\"郭靖\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel.wv.most_similar(\"黄蓉\", topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel.wv.most_similar(\"黄蓉道\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 寻找对应关系\n",
    "w2vmodel.wv.most_similar(['郭靖', '小红马'], ['黄药师'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel.wv.most_similar(positive=['郭靖', '黄蓉'], negative=['杨康'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算两个词的相似度/相关程度\n",
    "print(w2vmodel.wv.similarity(\"郭靖\", \"黄蓉\"))\n",
    "print(w2vmodel.wv.similarity(\"郭靖\", \"杨康\"))\n",
    "print(w2vmodel.wv.similarity(\"郭靖\", \"杨铁心\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 寻找不合群的词\n",
    "w2vmodel.wv.doesnt_match(\"小红马 黄药师 鲁有脚\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel.wv.doesnt_match(\"杨铁心 黄药师 黄蓉 洪七公\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档相似度\n",
    "### 基于词袋模型计算\n",
    "***sklearn实现***\n",
    ">sklearn.metrics.pairwise.pairwise_distances(\n",
    ">\n",
    ">X : 用于计算距离的数组\n",
    ">\n",
    ">  \\[n_samples_a, n_samples_a\\] if metric == 'precomputed'\n",
    ">\n",
    ">   \\[n_samples_a, n_features\\] otherwise\n",
    ">\n",
    ">Y = None : 用于计算距离的第二数组，当metric != 'precomputed'时可用\n",
    ">\n",
    ">metric = 'euclidean' : 空间距离计算方式\n",
    ">\n",
    ">scikit-learn原生支持 : \\['cityblock', 'cosine', 'euclidean', \n",
    ">\n",
    ">        'l1', 'l2', 'manhattan'\\]，可直接使用稀疏矩阵格式\n",
    ">\n",
    ">    来自scipy.spatial.distance : \\['braycurtis', 'canberra', \n",
    ">\n",
    ">    'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard',\n",
    ">\n",
    ">        'kulsinski', 'mahalanobis', 'matching', 'minkowski',\n",
    ">\n",
    ">        'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener',\n",
    ">\n",
    ">        'sokalsneath', 'sqeuclidean', 'yule'\\] 不支持稀疏矩阵格式\n",
    ">\n",
    ">n_jobs = 1 : 用于计算的线程数，为-1时，所有CPU内核都用于计算\n",
    ">\n",
    ">)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanchap = [ \" \".join(m_cut(w)) for w in chapter.txt.iloc[:5]] \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer() \n",
    "\n",
    "resmtx = countvec.fit_transform(cleanchap)\n",
    "resmtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "pairwise_distances(resmtx, metric = 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_distances(resmtx) # 默认值为euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用TF-IDF矩阵进行相似度计算\n",
    "pairwise_distances(tfidf[:5], metric = 'cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***gensim实现***\n",
    "\n",
    "***基于LDA计算余弦相似度***\n",
    "\n",
    "需要使用的信息：\n",
    "\n",
    "    拟合完毕的lda模型\n",
    "    按照拟合模型时矩阵种类转换的需检索文本\n",
    "        需检索的文本\n",
    "        建模时使用的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "simmtx = similarities.MatrixSimilarity(corpus)\n",
    "simmtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检索和第1章内容最相似（所属主题相同）的章节\n",
    "simmtx = similarities.MatrixSimilarity(corpus) # 使用的矩阵种类需要和拟合模型时相同\n",
    "simmtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simmtx.index[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用gensim的LDA拟合结果进行演示\n",
    "query = chapter.txt[1] \n",
    "query_bow = dictionary.doc2bow(m_cut(query))\n",
    "\n",
    "lda_vec = ldamodel[query_bow] # 转换为lda模型下的向量\n",
    "sims = simmtx[lda_vec] # 进行矩阵内向量和所提供向量的余弦相似度查询\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec\n",
    "word2vec用来计算词条相似度非常合适。\n",
    "\n",
    "较短的文档如果希望计算文本相似度，可以将各自内部的word2vec向量分别进行平均，用平均后的向量作为文本向量，从而用于计算相似度。\n",
    "\n",
    "但是对于长文档，这种平均的方式显然过于粗糙。\n",
    "\n",
    "doc2vec是word2vec的拓展，它可以直接获得sentences/paragraphs/documents的向量表达，从而可以进一步通过计算距离来得到sentences/paragraphs/documents之间的相似性。\n",
    "\n",
    "模型概况\n",
    "\n",
    "    分析目的：获得文档的一个固定长度的向量表达。\n",
    "    数据：多个文档，以及它们的标签，一般可以用标题作为标签。 \n",
    "    影响模型准确率的因素：语料的大小，文档的数量，越多越高；文档的相似性，越相似越好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba \n",
    "import gensim\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "def m_doc(doclist):\n",
    "    reslist = []\n",
    "    for i, doc in enumerate(doclist):\n",
    "        reslist.append(doc2vec.TaggedDocument(jieba.lcut(doc), [i]))\n",
    "    return reslist\n",
    "\n",
    "corp = m_doc(chapter.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2vmodel = gensim.models.Doc2Vec(vector_size = 300, \n",
    "                window = 20, min_count = 5)\n",
    "d2vmodel.build_vocab(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2vmodel.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将新文本转换为相应维度空间下的向量\n",
    "newvec = d2vmodel.infer_vector(jieba.lcut(chapter.txt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2vmodel.docvecs.most_similar([newvec], topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档聚类\n",
    "\n",
    "在得到文档相似度的计算结果后，文档聚类问题在本质上已经和普通的聚类分析没有区别。\n",
    "\n",
    "注意：最常用的Kmeans使用的是平方欧氏距离，这在文本聚类中很可能无法得到最佳结果。\n",
    "\n",
    "算法的速度和效果同样重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为章节增加名称标签\n",
    "chapter.index = [raw.txt[raw.chap == i].iloc[0] for i in chapter.index]\n",
    "chapter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "cuttxt = lambda x: \" \".join(m_cut(x)) \n",
    "cleanchap = chapter.txt.apply(cuttxt) \n",
    "cleanchap[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算TF-IDF矩阵\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer() \n",
    "wordmtx = vectorizer.fit_transform(cleanchap) # 将文本中的词语转换为词频矩阵  \n",
    "\n",
    "transformer = TfidfTransformer()  \n",
    "tfidf = transformer.fit_transform(wordmtx)  #基于词频矩阵计算TF-IDF值  \n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行聚类分析\n",
    "from sklearn.cluster import KMeans  \n",
    "\n",
    "clf = KMeans(n_clusters = 5)  \n",
    "s = clf.fit(tfidf)  \n",
    "print(s)  \n",
    "clf.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter['clsres'] = clf.labels_\n",
    "chapter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter.sort_values('clsres').clsres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapgrp = chapter.groupby('clsres')\n",
    "chapcls = chapgrp.agg(sum) # 只有字符串列的情况下，sum函数自动转为合并字符串\n",
    "\n",
    "cuttxt = lambda x: \" \".join(m_cut(x)) \n",
    "chapclsres = chapcls.txt.apply(cuttxt) \n",
    "chapclsres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列出关键词以刻画类别特征\n",
    "import jieba.analyse as ana\n",
    "\n",
    "ana.set_stop_words('停用词.txt')\n",
    "\n",
    "for item in chapclsres:\n",
    "    print(ana.extract_tags(item, topK = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文档分类\n",
    "## 什么是文本分类\n",
    "\n",
    "* ### 通过程序对文本按照一定的分类体系或标准自动分类标记\n",
    "\n",
    "* ### 应用场景\n",
    "   * 对抓取到的新闻进行自动归类\n",
    "   * 邮件服务器对收到的邮件进行垃圾邮件甄别\n",
    "   * 监测系统对采集到的文本信息进行优先级评估，将高优先级的信息优先发送至人工处理流程\n",
    "\n",
    "## 文本分类的基本步骤\n",
    "* ### 文本有效信息的提取\n",
    "   * 文本预处理：分词、清理等工作\n",
    "   * 特征抽取：从文档中抽取出反映文档主题的特征\n",
    "* ### 分类器的选择与训练\n",
    "\n",
    "* ### 分类结果的评价与反馈\n",
    "   * 该步骤与普通的模型完全相同\n",
    "   \n",
    "* ### 基于词袋模型时可考虑的特征抽取方法：\n",
    "   * 词频（基于词袋模型的文档-词条矩阵）\n",
    "   * 关键词（TF-IDF）\n",
    "   * 文档主题（LDA模型）\n",
    "   \n",
    "## 基于词袋模型的文本分类算法选择\n",
    "* ### 逻辑上所有用于分类因变量预测的模型都可以用于文本分类\n",
    "   * 判别分析、Logistics回归、树模型、神经网络、SVM、KNN、朴素贝叶斯、遗传算法、Bagging、Boosting\n",
    "   \n",
    "* ### 文本分类的数据特征\n",
    "   * 自变量（词条）数量极多\n",
    "   * 各自变量（词条）之间不可能完全独立\n",
    "   * 大部分自变量（词条）只是混杂项而已，对分类无贡献\n",
    "   \n",
    "* ### 选择算法的几个考虑方向\n",
    "   * 速度  变量筛选能力   容错性  共线性容忍度\n",
    "  \n",
    "  \n",
    "* ### 算法简单的贝叶斯公式（朴素贝叶斯）往往会成为优先考虑的算法\n",
    "\n",
    "* ### 模型中，改进后的随机森林，以及SVM相对应用较多\n",
    "\n",
    "* ### 有的算法会在特定数据集下表现较好，不能一概而论\n",
    "\n",
    "* ### 语料的事先清理至关重要，甚至可以考虑只使用关键词来分类\n",
    "\n",
    "## Python下的文本分类工具包选择\n",
    "\n",
    "* ### sklearn：\n",
    "   * 基于D2M矩阵结构，将文本分类问题看做标准的样本分类预测问题来处理\n",
    "   * 非常完善的模型拟合、诊断功能\n",
    "   * 同时也提供了朴素贝叶斯算法\n",
    "   \n",
    "* ### NLTK：\n",
    "   * 主要基于朴素贝叶斯算法进行文本分类\n",
    "   * 可以直接使用稀疏向量格式进行分析，使用上很方便\n",
    "\n",
    "* ### gensim：\n",
    "   * 基于LDA等更先进的模型，提供文本分类功能\n",
    "\n",
    "## 朴素贝叶斯的算法原理\n",
    "\n",
    "* ### 乘法公式：$P(AB) = P(B|A) * P(A) = P(A|B) * P(B)$\n",
    "   * $P(AB)$:联合概率\n",
    "   * $P(A)$或$P(B)$：先验概率\n",
    "   * $P(B|A)$或$P(A|B)$：后验概率\n",
    "   \n",
    "* ### 贝叶斯公式：$P(B|A) = \\frac{P(A|B) * P(B)}{P(A)}$\n",
    "   * 拥有某特征的案例，属于某类的概率 = 该类出现的概率 * 该类有某特征的概率 / 该特征出现的概率\n",
    "   \n",
    "* ### 该公式可以很容易的扩展至多条件的情况\n",
    "   * 具体公式大家可以自己尝试写出来\n",
    "   \n",
    "* ### 应用案例\n",
    "   * 判断一下邮件正文中含有句子：“我司可办理正规发票/增值税发票，点数优惠！”的是垃圾邮件的概率有多大？\n",
    "   \n",
    "* ### 句子的变化形式很多，但是核心信息就包含在“办理”、“发票”、“优惠”等几个词条的组合中，因此考虑分词后建模\n",
    "   * 发垃圾邮件的人也很敬业的......\n",
    "   \n",
    "* ### 朴素贝叶斯（Naive Bayes）=贝叶斯公式+条件独立假设\n",
    "   * 抛弃词条建的关联，假设各个词条完全独立，完全基于词袋模型进行计算\n",
    "   * 如此***蠢萌***的朴素贝叶斯方法，实践证明至少在垃圾邮件的识别中的应用效果是非常好的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn实现\n",
    "sklearn是标准的数据挖掘建模工具包，在语料转换为d2m矩阵结构之后，就可以使用所有标准的DM建模手段在sklearn中进行分析。\n",
    "\n",
    "在sklearn中也实现了朴素贝叶斯算法，使用方式上也和其他模型非常相似。\n",
    "\n",
    "***生成D2M矩阵***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从原始语料df中提取出所需的前两章段落\n",
    "raw12 = raw[raw.chap.isin([1,2])]\n",
    "raw12ana = raw12.iloc[list(raw12.txt.apply(len) > 50), :] # 只使用超过50字的段落\n",
    "raw12ana.reset_index(drop = True, inplace = True)\n",
    "print(len(raw12ana))\n",
    "raw12ana.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词和预处理\n",
    "import jieba\n",
    "\n",
    "cuttxt = lambda x: \" \".join(jieba.lcut(x)) # 这里不做任何清理工作，以保留情感词\n",
    "raw12ana[\"cleantxt\"] = raw12ana.txt.apply(cuttxt) \n",
    "raw12ana.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer() \n",
    "\n",
    "wordmtx = countvec.fit_transform(raw12ana.cleantxt)\n",
    "wordmtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***划分训练集和测试集***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作用：将数据集划分为 训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(wordmtx, raw12ana.chap, \n",
    "    test_size = 0.3, random_state = 111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***拟合朴素贝叶斯模型***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "NBmodel = naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拟合模型\n",
    "NBmodel.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行验证集预测\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBmodel.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***模型评估***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测准确率（给模型打分）\n",
    "print('训练集：', NBmodel.score(x_train, y_train), \n",
    "      '，验证集：', NBmodel.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, NBmodel.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***使用Logistic回归模型进行分类***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logitmodel = LogisticRegression() # 定义Logistic回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拟合模型\n",
    "logitmodel.fit(x_train, y_train)\n",
    "print(classification_report(y_test, logitmodel.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***模型预测***\n",
    "\n",
    "将需要预测的文本转换为和建模时格式完全对应的d2m矩阵格式，随后即可进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"杨铁心和包惜弱收养穆念慈\"\n",
    "words = \" \".join(jieba.lcut(string))\n",
    "words_vecs = countvec.transform([words]) # 数据需要转换为可迭代的list格式\n",
    "words_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBmodel.predict(words_vecs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK实现\n",
    "NLTK中内置了朴素贝叶斯算法，可直接实现文档分类。\n",
    "\n",
    "***数据集中语料的格式***\n",
    "\n",
    "用于训练的语料必须是分词完毕的字典形式，词条为键名，键值则可以是数值、字符、或者T/F\n",
    "\n",
    ">{'张三' : True, '李四' : True, '王五' : False}\n",
    ">\n",
    ">{'张三' : 1, '李四' : 1, '王五' : 0}\n",
    ">\n",
    ">{'张三' : '有', '李四' : '有', '王五' : '无'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Pandas的命令进行转换\n",
    "freqlist.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.groupby(['word']).agg('size').tail(10).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***训练用数据集的格式***\n",
    "训练用数据集为list of list格式，每个成员为list\\[语料字典, 结果变量\\]\n",
    "\n",
    ">\\[\n",
    ">\n",
    ">\\[{'张三' : 1, '李四' : 1, '王五' : 0}, '合格'\\],\n",
    ">\n",
    ">\\[{'张三' : 0, '李四' : 1, '王五' : 0}, '不合格'\\]\n",
    ">\n",
    ">\\]\n",
    "\n",
    "***构建模型***\n",
    "\n",
    "考虑到过拟合问题，此处需要先拆分好训练集和测试集\n",
    "\n",
    "model = NaiveBayesClassifier.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里直接以章节为一个单元进行分析，以简化程序结构\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "# 生成完整的词条频数字典，这部分也可以用遍历方式实现\n",
    "fdist1 = FreqDist(m_cut(chapter.txt[1])) \n",
    "fdist2 = FreqDist(m_cut(chapter.txt[2])) \n",
    "fdist3 = FreqDist(m_cut(chapter.txt[3])) \n",
    "fdist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "training_data = [ [fdist1, 'chap1'], [fdist2, 'chap2'], [fdist3, 'chap3'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练分类模型\n",
    "NLTKmodel = NaiveBayesClassifier.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NLTKmodel.classify(FreqDist(m_cut(\"杨铁心收养穆念慈\"))))\n",
    "print(NLTKmodel.classify(FreqDist(m_cut(\"钱塘江 日日夜夜 包惜弱 颜烈 使出杨家枪\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***模型拟合效果的考察***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.classify.accuracy(NLTKmodel, training_data) # 准确度评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLTKmodel.show_most_informative_features(5)#得到似然比，检测对于哪些特征有用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类结果评估\n",
    "## 精确率和召回率\n",
    "### 精确率和召回率主要用于二分类问题（从其公式推导也可看出），结合混淆矩阵有：\n",
    "\n",
    "|      |  分类1  | 分类2  |\n",
    "| :----: | :----: | :----: |\n",
    "| 预测分类1 | TP | FP |\n",
    "| 预测分类2 | FN | TN |\n",
    "\n",
    "### 精确率$Precision = \\frac{TP}{TP+FP}$\n",
    "\n",
    "### 召回率$Recall = \\frac{TP}{TP+FN}$\n",
    "\n",
    "\n",
    "理想情况下，精确率和召回率两者都越高越好。然而事实上这两者在某些情况下是矛盾的，精确率高时，召回率低；精确率低时，召回率高；关于这个性质通过观察PR曲线不难观察出来。比如在搜索网页时，如果只返回最相关的一个网页，那精确率就是100%，而召回率就很低；如果返回全部网页，那召回率为100%，精确率就很低。因此在不同场合需要根据实际需求判断哪个指标跟重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 1       0.67      1.00      0.80         2\n",
      "    class 2       0.00      0.00      0.00         1\n",
      "    class 3       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       0.67      0.80      0.72         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分类报告：precision/recall/fi-score/均值/分类个数\n",
    " from sklearn.metrics import classification_report\n",
    " class_true = [1, 2, 3, 3, 1] #正确的分类结果\n",
    " class_pred = [1, 1, 3, 3, 1] #实际的分类结果\n",
    " target_names = ['class 1', 'class 2', 'class 3']\n",
    " print(classification_report(class_true, class_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
